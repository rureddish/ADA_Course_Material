{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credits:** Jade Maï Cock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\"> \n",
    "        Command to make comments distinguishable\n",
    "</span>\n",
    "\n",
    "<span style=\"color:royalblue\">    \n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T06:42:36.226390Z",
     "start_time": "2020-01-14T06:42:08.597861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Manipulation Libraries\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed as random_seed\n",
    "from numpy.random import shuffle as random_shuffle\n",
    "import math\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "import os, codecs, string, random\n",
    "from numpy.random import randint\n",
    "\n",
    "# Visualisation Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from datetime import datetime, date, time\n",
    "from dateutil.parser import parse\n",
    "from pandas.plotting import scatter_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Web parsing Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Big Data Libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import jaccard\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# NLP Libraries\n",
    "import spacy, nltk, gensim, sklearn\n",
    "import pyLDAvis.gensim_models\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models.phrases import Phrases\n",
    "import re\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import itertools\n",
    "\n",
    "# Graphs\n",
    "import networkx as nx\n",
    "from operator import itemgetter\n",
    "from community import community_louvain\n",
    "import collections\n",
    "from networkx.algorithms.community.centrality import girvan_newman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data backups\n",
    "Dont forget to create a \"backup folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(index):\n",
    "    file = './backup/data_' + index + '.pkl'\n",
    "    with open(file, 'rb') as fp:\n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, index):\n",
    "    file = './backup/data_' + index + '.pkl'\n",
    "    with open(file, 'wb') as fp:\n",
    "        pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone\n",
    "index += 1\n",
    "save_data(bbt, str(index))\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">    \n",
    "    This function is used to check whether all the words of a list appear in a text    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T09:21:47.022498Z",
     "start_time": "2020-01-13T09:21:46.979877Z"
    }
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T10:22:53.206209Z",
     "start_time": "2020-01-13T10:22:53.188699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find word in text\n",
    "def word_in_text(words, text):\n",
    "    words = re.sub('s+','s*', '|'.join(words)) # Find pattern, replace with other, the list\n",
    "    text = text.lower()\n",
    "    match = re.search(words, text)\n",
    "    if match:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T10:22:54.522231Z",
     "start_time": "2020-01-13T10:22:54.516371Z"
    }
   },
   "outputs": [],
   "source": [
    "# Buffered Word\n",
    "hello = 'hello'\n",
    "world = 'world'\n",
    "string = r'\\b' + hello + r'\\b'\n",
    "string2 = r'\\b' + world + r'\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T10:23:03.916637Z",
     "start_time": "2020-01-13T10:23:03.910300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_in_text([string, string2], 'hello wrld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete substring untill certain character\n",
    "x : x[x.find(':'):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">    \n",
    "    Find all lines containing a word <br>\n",
    "    Counting tweets containing a word\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually apply the regexp\n",
    "keywords = ['open access', 'open science', 'ipython', 'open data', 'reproducible research','epfl']\n",
    "for w in keywords:\n",
    "    tweets[w] = tweets['text'].apply(lambda tweet: word_in_text([w], tweet))\n",
    "    \n",
    "tweets_by_kw = pd.Series([tweets[w].value_counts()[True] for w in keywords], index=keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">    \n",
    "    String functions\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T17:11:17.116555Z",
     "start_time": "2019-12-26T17:11:17.056572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'name_bacteria'.endswith('bacteria') # Check word ends with\n",
    "'name_bacteria'.startswith('bacteria') # Check word starts with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby function\n",
    "df.groupby('A').apply(lambda x: x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of occurences\n",
    "indices = [i for i, x in enumerate(my_list) if x == \"whatever\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">    \n",
    "    Save the data in intermediate steps\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T22:10:03.541213Z",
     "start_time": "2020-01-11T22:10:03.513458Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(index):\n",
    "    file = './backup/data_' + index + '.pkl'\n",
    "    with open(file, 'rb') as fp:\n",
    "        return pickle.load(fp)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, index):\n",
    "    file = './backup/data_' + index + '.pkl'\n",
    "    with open(file, 'wb') as fp:\n",
    "        pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone\n",
    "index += 1\n",
    "save_data(bbt, str(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">    \n",
    "    Retrieve statistical properties <br> \n",
    "    Gives: counte, mean, std, 25%, 50%, 75%, max\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">    \n",
    "    Plots functions to be used with dataframes\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='xcolumn', y='ycolumn')\n",
    "df['column'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">    \n",
    "    Standardise times\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating date\n",
    "datetime(1970, 1, 1) # yyyy - mm -dd\n",
    "\n",
    "# Parsing an entire column of date\n",
    "df.column.apply(lambda d: datetime.strptime(d, '%m/%d/%y %H:%M')).head(10)\n",
    "# Filter per month\n",
    "df[df.column.dt.month==2].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">    \n",
    "    Rotates dataframes:\n",
    "    <br> <br>\n",
    "    Stack:\n",
    "    * index 0, then secundary index column1, col2, col3,...\n",
    "    * index 1, then secundary index column1, col2, col3,...\n",
    "    <br><br>\n",
    "    Pivot:\n",
    "    * if several entities are represented on several rows, can split the columns into rows\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stack() # Columns into rows\n",
    "df.unstack() # Reverts the effect\n",
    "\n",
    "df_wide = df.pivot(index='id_column', columns='column to pivot', values='twstrs').head()\n",
    "\n",
    "df.explode(column=column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename\n",
    "df = df.rename({'Historical Significance': 'Role'}, axis = 1)\n",
    "# Set index\n",
    "df.set_index('index_column', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://worldtimeapi.org/api/timezone/Europe/Zurich')\n",
    "r.json()\n",
    "r.text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "r = requests.post('https://httpbin.org/post', data=payload)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "soup.h1\n",
    "soup.title.string\n",
    "all_links = soup.find_all('a')\n",
    "\n",
    "for link in all_links:\n",
    "    if(not link.get('href').startswith('http://dblp.uni-trier.de/')\n",
    "       and link.get('href').startswith('http')):  # just an example, you need more checks\n",
    "        external_links += 1\n",
    "        \n",
    "publications_wrappers = soup.find_all('li', class_='entry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T22:29:36.854001Z",
     "start_time": "2020-01-09T22:29:36.804870Z"
    }
   },
   "outputs": [],
   "source": [
    "colours = ['orchid', 'skyblue', 'darkmagenta', 'turquoise']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Colours](Screenshot_19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-10T15:34:49.641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Just Noticeable Difference\n",
    "def create_intervalled_colours(k, delta, n_colours):\n",
    "    colours = [0]\n",
    "    last_intensity = 1 / delta\n",
    "    for i in range(n_colours):\n",
    "        for j in range (last_intensity, 255 / delta, 1/delta):\n",
    "            if (j - last_intensity) / j == k or (j - last_intensity) / jy >= k:\n",
    "                last_intensity = j\n",
    "                colours.append(last_intensity)\n",
    "                break\n",
    "    return colours\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-log plot - Log axis with pyplot\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting uncertainty: error bars (SHOULD ALWAYS APPEAR ON GRAPHS\n",
    "* x : X positions\n",
    "* y : Y positions\n",
    "* xerr or yerr : positive numbers, symmetric from y or x. Can be shape (2, n) -> disymmetric errors or (,n), or n (for a constant error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting error bars\n",
    "plt.errorbar(x, y, xerr, yerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T17:04:15.630943Z",
     "start_time": "2020-01-11T17:03:53.150550Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# create the context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setMaster(\"local[*]\").setAll([\n",
    "                                   ('spark.executor.memory', '12g'),  # find\n",
    "                                   ('spark.driver.memory','4g'), # your\n",
    "                                   ('spark.driver.maxResultSize', '2G') # setup\n",
    "                                  ])\n",
    "# create the session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# create the context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT AirCraftType, count(*) MissionsCount\n",
    "FROM Bombing_Operations bo\n",
    "JOIN Aircraft_Glossary ag\n",
    "ON bo.AirCraft = ag.AirCraft\n",
    "GROUP BY AirCraftType\n",
    "ORDER BY MissionsCount DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping function\n",
    "liste.map(lambda x : x*2)\n",
    "\n",
    "# Filter function\n",
    "liste.filter(lambda x : x < 2)\n",
    "\n",
    "# Flatmap\n",
    "liste.flatmap(lambda x : [x, x*10])\n",
    "\n",
    "# Sample\n",
    "liste.sample(withReplacementBoolean, sampledFraction, seed)\n",
    "\n",
    "# Union\n",
    "liste.union(dataset2)\n",
    "\n",
    "# Intersection\n",
    "liste.intersection(dataset2)\n",
    "\n",
    "# Distinct\n",
    "liste.distinct()\n",
    "\n",
    "# Groupby\n",
    "dic.groupByKey()\n",
    "\n",
    "# Reduce\n",
    "dic.reduceByKey(sum)\n",
    "\n",
    "# Sort by key\n",
    "dic.sortByKey()\n",
    "\n",
    "# Join {(1,a), (2,b)}.join({(1,A), (1,X)}) → {(1, (a,A)), (1, (a,X))}\n",
    "dic.join(dic2)\n",
    "\n",
    "# collect - Return all elements of the dataset as an array\n",
    "data.collect()\n",
    "\n",
    "# Count elements in the dataset\n",
    "data.count()\n",
    "\n",
    "# Take the first n elements\n",
    "data.take(n)\n",
    "\n",
    "# Saving\n",
    "data.saveAsTextFile(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual global variables\n",
    "DATA_PATH = './data/'\n",
    "DEFAULT_ENCODING = 'UTF8'\n",
    "DEFAULT_COMPRESSION = 'gzip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">    \n",
    "    Looping through the files in a directory\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(path = DATA_PATH):\n",
    "    for file in listdir(path):\n",
    "        do_something(path + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Providing the target files through the command line\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--filename\", help=\"Name of the file to process\", type = str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid if key not in dic\n",
    "todo_list = collections.defaultdict(list)\n",
    "todo_list['ADA'].append('Homework 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid the initialisation\n",
    "counter = collections.Counter()\n",
    "counter['apples'] += 1\n",
    "counter['oranges'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "File name: exam.py\n",
    "Author: Jade Maï Cock - 294749\n",
    "Date created: 2020\n",
    "Date last modified: 2020\n",
    "Python Version: 3.6\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(param1):\n",
    "    '''\n",
    "     Function description.\n",
    "     :param param1: type\n",
    "     :return: result\n",
    "     '''\n",
    "    return bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "     print(factorial(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "def save_pickle(result, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(result, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattering plots to observe relationships between arguments\n",
    "data.plot(kind='scatter', x='newspaper', y='sales', ax=axs[2], grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation from categorical to numerical -> One hot encoding\n",
    "X =  pd.get_dummies(pokemon_features[pokemon_features.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min max scaler - Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "scaled_data = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaler - Scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "scaled_data = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lin_reg = LinearRegression()  # create the model\n",
    "lin_reg.fit(X, y)  # train it\n",
    "lin_reg.coef_ # Coefficient values for each attribute\n",
    "\n",
    "lr = LinearRegression()\n",
    "# Function for cross validation\n",
    "predicted = cross_val_predict(lr, X, y, cv=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs')\n",
    "precision = cross_val_score(logistic, X, y, cv=10, scoring=\"precision\")\n",
    "recall = cross_val_score(logistic, X, y, cv=10, scoring=\"recall\")\n",
    "\n",
    "\n",
    "\n",
    "logistic = LogisticRegression(solver='lbfgs')\n",
    "logistic.fit(X, y)\n",
    "logistic.predict([[25, 100, 0, 1]])\n",
    "logistic.predict_proba([[25, 100, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV(Cs=[], cv=10, random_state=0).fit(X, y)\n",
    "clf.predict(X[:2, :])\n",
    "clf.predict_proba(X[:2, :]).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbours - Classification\n",
    "Distance measures available:\n",
    "* euclidian\n",
    "* manhattan\n",
    "* chebyshev\n",
    "* minkowski (default)\n",
    "* wminkowski\n",
    "* seuclidean\n",
    "* mahalanobis\n",
    "* jaccard\n",
    "* hamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X, y)\n",
    "knn.predict([[test_instance]])\n",
    "knn.predict_proba([[0.9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbours - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=2)\n",
    "knn.fit(X, y)\n",
    "knn.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=n_estimator, max_depth=max_depth, random_state=0)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=n_estimator, max_depth=max_depth, random_state=0)\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        y_predict = clf.predict(X_test)\n",
    "        \n",
    "# Feature importance\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(10), importances[indices[0:10]], color=\"r\", yerr=std[indices[0:10]], align=\"center\")\n",
    "plt.xticks(range(10), indices[0:10])\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "cross_val_score(dt, iris.data, iris.target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T21:19:52.988155Z",
     "start_time": "2020-01-10T21:19:52.913429Z"
    }
   },
   "outputs": [],
   "source": [
    "# Entropy\n",
    "def H(p, n):\n",
    "    temp1 = (p / (p + n)) * np.log2(p / (p + n))\n",
    "    temp2 = (n / (p + n)) * np.log2(n / (p + n))\n",
    "    return - temp1 - temp2\n",
    "\n",
    "def entropy(dataframe, ps, ns):\n",
    "    '''\n",
    "    Dataframe should be two columns : \n",
    "    1: attribute to compute the entropy on, called \"attribute\"\n",
    "    2: label of the rows, called \"label\"\n",
    "    -> Labels are assumed to be 0 (negative) or 1 (positive)\n",
    "    '''\n",
    "    values = dataframe['attribute'].unique()    \n",
    "    entropy_value = 0\n",
    "    for val in values:\n",
    "        temp = dataframe[dataframe['attribute'] == val]\n",
    "        pos = len(temp[temp['label'] == 1])\n",
    "        neg = len(temp[temp['label'] == 0])\n",
    "        entropy_value += ((pos + neg) / (ps + ns)) * H(pos, neg)\n",
    "    return entropy_value\n",
    "\n",
    "def gain(dataframe):\n",
    "    ps = len(dataframe[dataframe['label'] == 1])\n",
    "    ns = len(dataframe[dataframe['label'] == 0])\n",
    "    \n",
    "    return H(ps, ns) - entropy(dataframe, ps, ns)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous Features\n",
    "coefficient = pearsonr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features and label\n",
    "# Estimated mutual information between each feature and the target\n",
    "mutual_info_classif(X, y, discrete_features='auto', n_neighbors=3, \n",
    "                    copy=True, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE: Mean squared error\n",
    "mean_squared_error(y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision cross cal\n",
    "precision = cross_val_score(logistic, X, y, cv=10, scoring=\"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall cross val\n",
    "recall = cross_val_score(logistic, X, y, cv=10, scoring=\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 scores - R² Scores : 0-> Bad model, 1-> Good model \n",
    "r2_score(y_true, y_pred, sample_weight=None, multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function - Categorical\n",
    "def get_cat_loss(y_pred, y_true):\n",
    "    '''\n",
    "    y_pred : np.array\n",
    "    y_true : np.array\n",
    "    '''\n",
    "    return (1 / len(y_pred)) * np.sum(y_pred != y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared error - Real valued output\n",
    "def get_squarred_errot(y_pred, y_true):\n",
    "    '''\n",
    "    y_pred: np.array\n",
    "    y_true : np.array\n",
    "    '''\n",
    "    temp = y_true - y_pred\n",
    "    temp = temp ** 2\n",
    "    temp = np.sum(temp)\n",
    "    return (1 / len(y_pred)) * temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute error - Real valued output\n",
    "def get_absolute_error(y_pred, y_true):\n",
    "    '''\n",
    "    y_pred: np.array\n",
    "    y_true : np.array\n",
    "    '''\n",
    "    temp = y_true - y_pred\n",
    "    temp = np.abs(temp)\n",
    "    temp = np.sum(temp)\n",
    "    return (1 / len(y_pred)) * temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def get_accuracy(tp, tn, fp, fn):\n",
    "    return (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "# Precision\n",
    "def get_precision(tp, tn, fp, fn):\n",
    "    return (tp) / (tp + fp)\n",
    "\n",
    "# Recall\n",
    "def get_recall(tp, tn, fp, fn):\n",
    "    return (tp) / (tp + fn)\n",
    "\n",
    "# F1 score - F score\n",
    "def get_f1score(tp, tn, fp, fn):\n",
    "    p = get_precision(tp, tn, fp, fn)\n",
    "    r = get_recall(tp, tn, fp, fn)\n",
    "    \n",
    "    return 2 * ((p * r) / (p + r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC plots\n",
    "def get_tf_pn(y_preds, y_true):\n",
    "    y_p = pd.DataFrame(y_preds)\n",
    "    y_t = pd.DataFrame(y_true)\n",
    "    ys = pd.concat([y_p, y_t], axis = 1)\n",
    "    ys.columns = ['preds', 'true']\n",
    "    \n",
    "    temp_t = ys[ys['preds'] == ys['true']]\n",
    "    tp = len(temp_t[temp_t['true'] == 1]) \n",
    "    tn = len(temp_t[temp_t['true'] == 0])\n",
    "    \n",
    "    temp_f = ys[ys['preds'] != ys['true']]\n",
    "    fp = len(temp_f[temp_f['preds'] == 1])\n",
    "    fn = len(temp_f[temp_f['preds'] == 0])\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "\n",
    "def get_roc_axisplots(ypreds, y_true):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(0, 1, 0.05):\n",
    "        y_labelled = y_preds > i\n",
    "        y_labelled = y_labelled.astype(int)\n",
    "        tp, tn, fp, fn = get_tf_pn(y_labelled, y_true)\n",
    "        \n",
    "        x = fp / (fp + tn)\n",
    "        y = tp / (tp + fn)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        \n",
    "    return xs, ys\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=6)\n",
    "predicted_r = cross_val_predict(ridge, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prediction, residuals and least square solution\n",
    "# y : true labels   ---   predicted : values predicted by the algorithm\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(y, predicted, edgecolors=(0, 0, 0))\n",
    "ax.plot([min(y), max(y)], [min(y), max(y)], 'r--', lw=4)\n",
    "ax.set_xlabel('Original')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "euc_dist = euclidean(x, y, w=None)\n",
    "\n",
    "# Cosine distance\n",
    "cos_dist = cosine(x, y, w=None)\n",
    "\n",
    "# Jaccard distance\n",
    "jac_dist = jaccard(x, y, w=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means algorithm\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "kmeans.labels_ # Cluters created\n",
    "kmeans.predict([[0, 0], [12, 3]])\n",
    "kmeans.cluster_centers_ # Centers of the predicted clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN \n",
    "dbscan = DBSCAN(eps=3, min_samples=2).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point wise mutual information PMI\n",
    "def pwmi(pc, pw, pcw):\n",
    "    return pcw / (pc  * pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">    \n",
    "    All functions here are related to language processing on a series of books. <br>\n",
    "    The variable \"books\" is considered to be a list of several books\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a book\n",
    "with codecs.open(path, encoding=\"utf8\") as f:\n",
    "    books.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete new lines\n",
    "books = [\" \".join(b.split()) for b in books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into raw text / spacy object\n",
    "doc = nlp(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens\n",
    "tokens = [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Lemmas\n",
    "for token in doc:\n",
    "    print(token.text,'--->',token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of Speech Tagging - POS\n",
    "# Tag is more specific\n",
    "pos_tagged = [(token.text, token.pos_, token.tag_) for token in doc]\n",
    "\n",
    "print(example,'\\n')\n",
    "print(pos_tagged)\n",
    "\n",
    "print(spacy.explain('CCONJ')) # Explanation of a grammatical class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequent words\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams\n",
    "bigram = Phrases(docs, min_count=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF ~ tf idf\n",
    "tfids_vec = TfidfVectorizer()\n",
    "X = tfids_vec.fit_transform(corpus)# Corpus : list of strings\n",
    "print(tfids_vec.get_feature_names())\n",
    "\n",
    "# Bag of Words - Count vectoriser\n",
    "bow_vec = CountVectorizer(ngram_range=(2, 2))\n",
    "X = bow_vec.fit_transform(corpus)\n",
    "print(bow_vec.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Semantic Analysis - LSA\n",
    "lsa = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "lsa.fit(X)\n",
    "print(lsa.explained_variance_ratio_)\n",
    "print(lsa.explained_variance_ratio_.sum())\n",
    "print(lsa.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Dirichlet Allocation - LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X, y)\n",
    "print(lda.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Types\n",
    "* PERSON People, including fictional.\n",
    "* NORP Nationalities or religious or political groups.\n",
    "* FAC Buildings, airports, highways, bridges, etc.\n",
    "* ORG Companies, agencies, institutions, etc.\n",
    "* GPE Countries, cities, states.\n",
    "* LOC Non-GPE locations, mountain ranges, bodies of water.\n",
    "* PRODUCT Objects, vehicles, foods, etc. (Not services.)\n",
    "* EVENT Named hurricanes, battles, wars, sports events, etc.\n",
    "* WORK_OF_ART Titles of books, songs, etc.\n",
    "* LAW Named documents made into laws.\n",
    "* LANGUAGE Any named language.\n",
    "* DATE Absolute or relative dates or periods.\n",
    "* TIME Times smaller than a day.\n",
    "* PERCENT Percentage, including \"%\".\n",
    "* MONEY Monetary values, including unit.\n",
    "* QUANTITY Measurements, as of weight or distance.\n",
    "* ORDINAL \"first\", \"second\", etc.\n",
    "* CARDINAL Numerals that do not fall under another type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse many docs\n",
    "docs = list(nlp.pipe(df.Line, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stop_words = [token.text for token in doc if token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance\n",
    "variance = np.var(x)\n",
    "\n",
    "# Standard Deviation\n",
    "std = np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random number \n",
    "seed = randint(low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstraps - Bootstrapping - list\n",
    "mean_bootstraps = []\n",
    "yerrs_list1 = []\n",
    "yerrs_list2 = []\n",
    "bootstrap_datasets = []\n",
    "for bootstrap_dataset in df['attribute'].unique():\n",
    "    bootstrap_datasets.append(bootstrap_dataset)\n",
    "    bootstrap_scores = []\n",
    "    n = 1000\n",
    "    df = df[df['attribute'] == bootstrap_dataset]\n",
    "    for i in range(n):\n",
    "        # Extraction of the sample\n",
    "        indices_sample = np.random.choice(list(range(len(df))), \n",
    "                                          len(df), replace=True)\n",
    "        df_sample = df.iloc[indices_sample] \n",
    "        \n",
    "        # Bootstrap list\n",
    "        bootstrap_scores.append(np.mean(df_sample['nr_words']))\n",
    "\n",
    "\n",
    "    # Computation on the whole dataset\n",
    "    bootstrap_scores = np.sort(bootstrap_scores)\n",
    "    mean_bootstrap = np.mean(bootstrap_scores)\n",
    "    mean_bootstraps.append(mean_bootstrap)\n",
    "    lower_bootstrap = (bootstrap_scores - mean_bootstrap)[int(n * 0.025)]\n",
    "    higher_bootstrap = (bootstrap_scores - mean_bootstrap)[int(n * 0.025)]\n",
    "\n",
    "    yerrs = [(lower_bootstrap), (higher_bootstrap)]\n",
    "    yerrs_list1.append(yerrs[0])\n",
    "    yerrs_list2.append(yerrs[1])\n",
    "\n",
    "\n",
    "yerrs_list = [yerrs_list1, yerrs_list2]\n",
    "plt.figure(figsize=(25, 8))\n",
    "# Can change mean and yerrs to array for several barplots\n",
    "plt.bar(x = list(range(19)), height = mean_bootstraps, yerr=yerrs_list, color='paleturquoise')\n",
    "plt.xticks(list(range(19)), bootstrap_datasets)\n",
    "plt.xlim(-0.5, 20.5)\n",
    "plt.title('')\n",
    "plt.xlabel('Mean (*) and 95% confidence interval (-)')\n",
    "plt.ylabel(\"Mean square error from the bootstrap sampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstraps - Bootstrapping - One value\n",
    "bootstrap_scores = []\n",
    "n = 10000\n",
    "n_samples = 56\n",
    "for i in range(n):\n",
    "    # Extraction of the sample\n",
    "    indices_sample_grass = np.random.choice(list(range(len(grasses))), \n",
    "                                      n_samples, replace=True)\n",
    "    indices_sample_rock = np.random.choice(list(range(len(rocks))), \n",
    "                                      n_samples, replace=True)\n",
    "    grass_sample = grasses.iloc[indices_sample_grass] \n",
    "    rock_sample = rocks.iloc[indices_sample_rock] \n",
    "    \n",
    "    stat, p = ttest_ind(grass_sample[\"attack\"], rock_sample[\"attack\"], axis=0, \n",
    "                        equal_var=True, nan_policy='propagate')\n",
    "    # Bootstrap list\n",
    "    bootstrap_scores.append(p)\n",
    "\n",
    "\n",
    "# Computation on the whole dataset\n",
    "bootstrap_scores = np.sort(bootstrap_scores)\n",
    "mean_bootstrap = np.mean(bootstrap_scores)\n",
    "lower_bootstrap = (bootstrap_scores - mean_bootstrap)[int(n * 0.025)]\n",
    "higher_bootstrap = (bootstrap_scores - mean_bootstrap)[int(n * 0.025)]\n",
    "\n",
    "std = np.std(bootstrap_scores)\n",
    "\n",
    "yerrs = [[lower_bootstrap, mean_bootstrap - std], [higher_bootstrap, mean_bootstrap + std]]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Can change mean and yerrs to array for several barplots\n",
    "plt.bar(x = [0, 1], height = mean_bootstrap, yerr=yerrs, color='paleturquoise')\n",
    "plt.xticks([0, 1], ['95% confidence interval', 'STD'])\n",
    "plt.xlabel('p-value of the t test between the attack value of grass and rock pokemons')\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.title('Bootstrapped p-Value')\n",
    "plt.xlabel('Mean (*) and 95% confidence interval (-)')\n",
    "plt.ylabel(\"Mean square error from the bootstrap sampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = ttest_ind(a, b, axis=0, equal_var=True, nan_policy='propagate')[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = wilcoxon(x, y=None, zero_method='wilcox', correction=False, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, p = kstest(rvs, cdf, args=(), N=20, alternative='two-sided', mode='approx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph\n",
    "G = nx.Graph() # undirected\n",
    "di_G = nx.DiGraph() # directed\n",
    "\n",
    "# Add Nodes\n",
    "G.add_node(1)\n",
    "G.add_nodes_from(range(2,9)\n",
    "\n",
    "# Add Edges\n",
    "G.add_edge(1,2)\n",
    "edges = [(2,3), (1,3), (4,1), (4,5), (5,6), (5,7), (6,7), (7,8), (6,8)]\n",
    "G.add_edges_from(edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edges from pandas\n",
    "G = nx.from_pandas_edgelist(edges, 'Source', 'Target', \n",
    "                                  edge_attr=None, create_using = nx.Graph())\n",
    "\n",
    "# Add nodes from pandas\n",
    "# add node attributes by passing dictionary of type name -> attribute\n",
    "nx.set_node_attributes(G, nodes['attributes'].to_dict(), 'attributes' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From edge list\n",
    "# 2 columns : Source, Target\n",
    "quakerG = nx.from_pandas_edgelist(edges, 'Source', 'Target', edge_attr=None, create_using= nx.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Graph\n",
    "erG = nx.gnm_random_graph(n, m) # n:nodes - m:edges\n",
    "describe_graph(erG)\n",
    "visualize_graph(erG, k=0.05, alpha=0.6)\n",
    "plot_degree_distribution(erG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Nodes\n",
    "G.nodes()\n",
    "\n",
    "# Get Information\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Graph\n",
    "nx.draw_spring(G, with_labels=True,  alpha = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T20:01:12.510442Z",
     "start_time": "2020-01-11T20:01:12.486254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Degree distrbution\n",
    "def plot_degree_distribution(G):\n",
    "    degrees = {}\n",
    "    for node in G.nodes():\n",
    "        degree = G.degree(node)\n",
    "        if degree not in degrees:\n",
    "            degrees[degree] = 0\n",
    "        degrees[degree] += 1\n",
    "    sorted_degree = sorted(degrees.items())\n",
    "    deg = [k for (k,v) in sorted_degree]\n",
    "    cnt = [v for (k,v) in sorted_degree]\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(deg, cnt, width=0.80, color='plum')\n",
    "    plt.title(\"Degree Distribution\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"Degree\")\n",
    "    ax.set_xticks([d+0.05 for d in deg])\n",
    "    ax.set_xticklabels(deg)\n",
    "    \n",
    "# Graph properties\n",
    "def describe_graph(G):\n",
    "    print(nx.info(G))\n",
    "    if nx.is_connected(G):\n",
    "        print(\"Avg. Shortest Path Length: %.4f\" %nx.average_shortest_path_length(G))\n",
    "        print(\"Diameter: %.4f\" %nx.diameter(G)) # Longest shortest path\n",
    "    else:\n",
    "        print(\"Graph is not connected\")\n",
    "        print(\"Diameter and Avg shortest path length are not defined!\")\n",
    "    print(\"Sparsity: %.4f\" %nx.density(G))  # #edges/#edges-complete-graph\n",
    "    # #closed-triplets(3*#triangles)/#all-triplets\n",
    "    print(\"Global clustering coefficient aka Transitivity: %.4f\" %nx.transitivity(G))\n",
    "    \n",
    "# Helper function for visualizing the graph\n",
    "def visualize_graph(G, with_labels=True, k=None, alpha=0.6, node_shape='o'):\n",
    "    #nx.draw_spring(G, with_labels=with_labels, alpha = alpha)\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    pos = nx.spring_layout(G, k=k)\n",
    "    if with_labels:\n",
    "        lab = nx.draw_networkx_labels(G, pos, labels=dict([(n, n) for n in G.nodes()]))\n",
    "    ec = nx.draw_networkx_edges(G, pos, alpha=alpha)\n",
    "    nc = nx.draw_networkx_nodes(G, pos, nodelist=G.nodes(), node_color='powderblue', node_shape=node_shape)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circular graph\n",
    "nx.draw_circular(G, with_labels=True,  node_color='powderblue', alpha = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity\n",
    "print(\"Network sparsity: %.4f\" %nx.density(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if connected graph\n",
    "print(nx.is_connected(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of components\n",
    "comp = list(nx.connected_components(G))\n",
    "print('The graph contains', len(comp), 'connected components')\n",
    "\n",
    "# Largest component\n",
    "largest_comp = max(comp, key=len)\n",
    "percentage_lcc = len(largest_comp)/G.number_of_nodes() * 100\n",
    "print('The largest component has', len(largest_comp), 'nodes', \n",
    "      'accounting for %.2f'% percentage_lcc, '% of the nodes') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortest path\n",
    "path = nx.shortest_path(G, source=\"source node\", target=\"target node\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longest short path - diameter of the graph\n",
    "temp_graph = graph.subgraph(largest_comp)\n",
    "print(\"The diameter of the largest connected component is\", \n",
    "      nx.diameter(lcc_quakerG))\n",
    "print(\"The avg shortest path length of the largest connected component is\", \n",
    "      nx.average_shortest_path_length(lcc_quakerG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of all possible triangles - Transitivity - Triadic closure\n",
    "print('%.4f' %nx.transitivity(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering coefficient\n",
    "print(nx.clustering(G, ['node1', 'node2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All pairs shortest path length\n",
    "all_pairs_shortest_path_length(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgraph\n",
    "subgraph_node1 = G.subgraph(['Node 1'] + list(G.neighbors('Node 1')))\n",
    "nx.draw_spring(G, with_labels=True)\n",
    "nx.draw_circular(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute degrees and see importance\n",
    "degrees = dict(G.degree(G.nodes()))\n",
    "sorted_degree = sorted(degrees.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "# And the top 5 most popular quakers are.. \n",
    "for nodeName, degree in sorted_degree[:5]:\n",
    "    print(nodeName, 'who is', G.node[nondeName]['Role'], 'knows', degree, 'people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree Distribution Scatter plot\n",
    "degree_seq = [d[1] for d in sorted_degree]\n",
    "degreeCount = collections.Counter(degree_seq)\n",
    "degreeCount = pd.DataFrame.from_dict( degreeCount, orient='index').reset_index()\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(degreeCount['index'], degreeCount[0], 'o', c='blue', markersize= 4)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Degree')\n",
    "plt.title('Degree distribution for the Quaker network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree of Centrality Katz\n",
    "degrees = dict(G.degree(G.nodes()))\n",
    "\n",
    "katz = nx.katz_centrality(G)\n",
    "nx.set_node_attributes(G, katz, 'katz')\n",
    "sorted_katz = sorted(katz.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "# And the top 5 most popular quakers are.. \n",
    "for nodeName, katzc in sorted_katz[:5]:\n",
    "    print(nodeName, 'who is', G.node[nodeName]['Role'], 'has katz-centrality: %.3f' %katzc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute betweenness centrality\n",
    "betweenness = nx.betweenness_centrality(G)\n",
    "# Assign the computed centrality values as a node-attribute in your network\n",
    "nx.set_node_attributes(G, betweenness, 'betweenness')\n",
    "sorted_betweenness = sorted(betweenness.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "for nodeName, bw in sorted_betweenness[:5]:\n",
    "    print(nodeName, 'who is', G.node[nodeName]['Role'], 'has betweeness: %.3f' %bw)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree centrality heatmap\n",
    "# similar pattern\n",
    "list_nodes =list(G.nodes())\n",
    "list_nodes.reverse()   # for showing the nodes with high betweeness centrality \n",
    "pos = nx.spring_layout(G)\n",
    "ec = nx.draw_networkx_edges(G, pos, alpha=0.1)\n",
    "nc = nx.draw_networkx_nodes(G, pos, nodelist=list_nodes, \n",
    "                            node_color=[G.nodes[n][\"betweenness\"] for n in list_nodes], \n",
    "                            with_labels=False, alpha=0.8, node_shape = '.')\n",
    "plt.colorbar(nc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girvan Newman\n",
    "comp = girvan_newman(G)\n",
    "it = 0\n",
    "for communities in itertools.islice(comp, 4):\n",
    "    it +=1\n",
    "    print('Iteration', it)\n",
    "    print(tuple(sorted(c) for c in communities)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Louvain\n",
    "partition = community_louvain.best_partition(G)\n",
    "# add it as an attribute to the nodes\n",
    "for n in G.nodes:\n",
    "    G.nodes[n][\"louvain\"] = partition[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G, k=0.2)\n",
    "ec = nx.draw_networkx_edges(G, pos, alpha=0.2)\n",
    "nc = nx.draw_networkx_nodes(G, pos, nodelist=G.nodes(), \n",
    "                            node_color=[G.nodes[n][\"louvain\"] for n in G.nodes], \n",
    "                            with_labels=False, node_size=100, cmap=plt.cm.jet)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterNode = partition['Node']\n",
    "# Take all the nodes that belong to James' cluster\n",
    "members_c = [q for q in G.nodes if partition[q] == clusterNode]\n",
    "# get info about these quakers\n",
    "for nodeName in members_c:\n",
    "    print(nodeName, 'who is', G.node[nodeName]['Role'], 'and died in ',\n",
    "          G.node[quaker]['Deathdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homophily\n",
    "# for categorical attributes\n",
    "nx.attribute_assortativity_coefficient(G, 'Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.groupby('Gender').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if edge\n",
    "G.has_edge(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digraph API\n",
    "DiGraph.has_edge(u, v)\n",
    "https://networkx.github.io/documentation/networkx-1.10/reference/classes.digraph.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  4 2022, 08:45:25) [Clang 12.0.0 ]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "236.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "10d059aaca96fc9c3493bab5d1c0141ba04039945f551b22f762be411355277a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
