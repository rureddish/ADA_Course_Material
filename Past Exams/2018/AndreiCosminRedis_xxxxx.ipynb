{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Manipulation Libraries\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed as random_seed\n",
    "from numpy.random import shuffle as random_shuffle\n",
    "import math\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "import os, codecs, string, random\n",
    "from numpy.random import randint\n",
    "\n",
    "#Visualisation Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from datetime import datetime, date, time\n",
    "from dateutil.parser import parse\n",
    "from pandas.plotting import scatter_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# # Web parsing Libraries\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # Big Data Libraries\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import *\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark import SparkContext\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import jaccard\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# NLP Libraries\n",
    "import spacy, nltk, gensim, sklearn\n",
    "import pyLDAvis.gensim_models\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models.phrases import Phrases\n",
    "import re\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import itertools\n",
    "\n",
    "# # Graphs\n",
    "# import networkx as nx\n",
    "# from operator import itemgetter\n",
    "# from community import community_louvain\n",
    "# import collections\n",
    "# from networkx.algorithms.community.centrality import girvan_newman\n",
    "\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "EXCLUDE_CHARS = set(punctuation).union(set('’'))\n",
    "def simple_tokeniser(text):\n",
    "    return text.split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Significant Bang Theory\n",
    "\n",
    "Attention, ADA students!\n",
    "\n",
    "The Sheldon Cooper we all know and love (OK, some of us might not know him, and some might not love him) from the TV series \"The Big Bang Theory\" has gotten into an argument with Leonard from the same TV show. Sheldon insists that he knows the show better than anyone, and keeps making various claims about the show, which neither of them know how to prove or disprove. The two of them have reached out to you ladies and gentlemen, as data scientists, to help them. You will be given the full script of the series, with information on the episode, the scene, the person saying each dialogue line, and the dialogue lines themselves.\n",
    "\n",
    "Leonard has challenged several of Sheldon's claims about the show, and throughout this exam you will see some of those and you will get to prove or disprove them, but remember: **sometimes, we can neither prove a claim, nor disprove it!**\n",
    "\n",
    "## Deadline\n",
    "Wednesday, January 30th, 2019; 11:15 A.M. (Swiss time)\n",
    "\n",
    "_For the deadline for extramural exams, see the submission subsection._\n",
    "\n",
    "## Important notes\n",
    "* Don't forget to add a textual description of your thought process, the assumptions you made, and your results!\n",
    "* Please write all your comments in English, and use meaningful variable names in your code.\n",
    "* As we have seen during the semester, data science is all about multiple iterations on the same dataset. Do not obsess over small details in the beginning, and try to complete as many tasks as possible during the first 2 hours. Then, go back to the obtained results, write meaningful comments, and debug your code if you have found any glaring mistake.\n",
    "* Fully read the instructions for each question before starting to solve it to avoid misunderstandings, and remember to save your notebook often!\n",
    "* The exam contains **15 questions organised into 4 tasks**, and is designed for more than 3 hours. **You do not need to solve everything in order to get a 6**, and you have some freedom is choosing the tasks you wish to solve.\n",
    "* You cannot leave the room in the first and last 15 minutes.\n",
    "* You can use all the online resources you want except for communication tools (emails, web chats, forums, phone, etc.). We will be monitoring the network for unusual activity.\n",
    "* Remember, this is not a homework assignment -- no teamwork allowed!\n",
    "\n",
    "## Submission\n",
    "* Your file has to be named as \"NameSurname_SCIPER.ipynb\".\n",
    "* Make sure you upload your Jupyter Notebook (1 file) to [this](https://goo.gl/forms/7GLvYl94uSOn54jH2) Google form at the end of the exam, with all the cells already evaluated (except for the Spark-related question, Q7). You need to sign in to Google using your EPFL credentials in order to submit the form.\n",
    "* In case of problems with the form, send your Jupyter Notebook (along with your name and SCIPER number) as a direct message to @ramtin on Mattermost. This is reserved only for those who encounter problems with the submission -- you need to have a reasonable justification for using this back-up.\n",
    "* You will have until 11:20 (strict deadline) to turn in your submission. **Late submissions will not be accepted.** This deadline is for the students taking the exam at EPFL -- students taking the exam extramurally will have their submission deadline as the starting time of the exam plus 3 hours and 5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task A: Picking up the shovel (10 points)\n",
    "\n",
    "**Note: You will use the data you preprocess in this task in all the subsequent ones.**\n",
    "\n",
    "Our friends' argument concerns the entire show. We have given you a file in the `data/` folder that contains the script of every single episode. New episodes are indicated by '>>', new scenes by '>', and the rest of the lines are dialogue lines. Some lines are said by multiple people (for example, lines indicated by 'All' or 'Together'); **you must discard these lines**, for the sake of simplicity. However, you do not need to do it for Q1 in this task -- you'll take care of it when you solve Q2.\n",
    "\n",
    "**Q1**. (5 points) Your first task is to extract all lines of dialogue in each scene and episode, creating a dataframe where each row has the episode and scene where a dialogue line was said, the character who said it, and the line itself. You do not need to extract the proper name of the episode (e.g. episode 1 can appear as \"Series 01 Episode 01 - Pilot Episode\", and doesn't need to appear as \"Pilot Episode\"). Then, answer the following question: In total, how many scenes are there in each season? We're not asking about unique scenes; the same location appearing in two episodes counts as two scenes. You can use a Pandas dataframe with a season column and a scene count column as the response.\n",
    "\n",
    "**Note: The data refers to seasons as \"series\".**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dataframe where each row has the episode and scene where a dialogue line was said, the character who said it, and the line itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\">> Series 01 Episode 01 – Pilot Episode\"[10:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = './data/'\n",
    "lines = open(data_folder+'all_scripts.txt', 'r')\n",
    "char = ''\n",
    "scene = ''\n",
    "episode = ''\n",
    "season = ''\n",
    "df = []\n",
    "for line in lines:\n",
    "    if line.startswith('>> '):\n",
    "        season = int(line[10:12])\n",
    "        episode = int(line[21:23])\n",
    "    elif line.startswith('> '):\n",
    "        scene = line[2:]\n",
    "    else:\n",
    "        character, line = line.split(':', 1)\n",
    "        df.append([line.strip(),character,scene.strip(),episode,season])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "      <th>Character</th>\n",
       "      <th>Scene</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So if a photon is directed through a plane wit...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agreed, what’s your point?</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There’s no point, I just think it’s a good ide...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excuse me?</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hang on.</td>\n",
       "      <td>Receptionist</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51287</th>\n",
       "      <td>Mmm. No big deal, I enjoy spending time with you.</td>\n",
       "      <td>Ramona</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51288</th>\n",
       "      <td>And I with you. Question, are you seeking a ro...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51289</th>\n",
       "      <td>What if I were?</td>\n",
       "      <td>Ramona</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51290</th>\n",
       "      <td>Well, that would raise a number of problems. W...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51291</th>\n",
       "      <td>(Knock, knock, knock) Amy. (Knock, knock, knoc...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Princeton.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51292 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Line     Character  \\\n",
       "0      So if a photon is directed through a plane wit...       Sheldon   \n",
       "1                             Agreed, what’s your point?       Leonard   \n",
       "2      There’s no point, I just think it’s a good ide...       Sheldon   \n",
       "3                                             Excuse me?       Leonard   \n",
       "4                                               Hang on.  Receptionist   \n",
       "...                                                  ...           ...   \n",
       "51287  Mmm. No big deal, I enjoy spending time with you.        Ramona   \n",
       "51288  And I with you. Question, are you seeking a ro...       Sheldon   \n",
       "51289                                    What if I were?        Ramona   \n",
       "51290  Well, that would raise a number of problems. W...       Sheldon   \n",
       "51291  (Knock, knock, knock) Amy. (Knock, knock, knoc...       Sheldon   \n",
       "\n",
       "                             Scene  Episode  Season  \n",
       "0      A corridor at a sperm bank.        1       1  \n",
       "1      A corridor at a sperm bank.        1       1  \n",
       "2      A corridor at a sperm bank.        1       1  \n",
       "3      A corridor at a sperm bank.        1       1  \n",
       "4      A corridor at a sperm bank.        1       1  \n",
       "...                            ...      ...     ...  \n",
       "51287            Sheldon’s office.       24      10  \n",
       "51288            Sheldon’s office.       24      10  \n",
       "51289            Sheldon’s office.       24      10  \n",
       "51290            Sheldon’s office.       24      10  \n",
       "51291                   Princeton.       24      10  \n",
       "\n",
       "[51292 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df, columns=['Line', 'Character', 'Scene', 'Episode', 'Season'])\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the scenes in seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "      <th>Character</th>\n",
       "      <th>Scene</th>\n",
       "      <th>Episode</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Season</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3831</td>\n",
       "      <td>34</td>\n",
       "      <td>139</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4825</td>\n",
       "      <td>34</td>\n",
       "      <td>159</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4704</td>\n",
       "      <td>44</td>\n",
       "      <td>124</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5172</td>\n",
       "      <td>46</td>\n",
       "      <td>131</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4559</td>\n",
       "      <td>52</td>\n",
       "      <td>134</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4551</td>\n",
       "      <td>41</td>\n",
       "      <td>127</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4894</td>\n",
       "      <td>47</td>\n",
       "      <td>114</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4870</td>\n",
       "      <td>44</td>\n",
       "      <td>107</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5055</td>\n",
       "      <td>46</td>\n",
       "      <td>89</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5176</td>\n",
       "      <td>45</td>\n",
       "      <td>90</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Line  Character  Scene  Episode\n",
       "Season                                 \n",
       "1       3831         34    139       17\n",
       "2       4825         34    159       23\n",
       "3       4704         44    124       23\n",
       "4       5172         46    131       24\n",
       "5       4559         52    134       24\n",
       "6       4551         41    127       24\n",
       "7       4894         47    114       24\n",
       "8       4870         44    107       24\n",
       "9       5055         46     89       24\n",
       "10      5176         45     90       24"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Season').nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**. (5 points) Now, let's define two sets of characters: all the characters, and recurrent characters. Recurrent characters are those who appear in more than one episode. For the subsequent sections, you will need to have a list of recurrent characters. Assume that there are no two _named characters_ (i.e. characters who have actual names and aren't referred to generically as \"little girl\", \"grumpy grandpa\", etc.) with the same name, i.e. there are no two Sheldons, etc. Generate a list of recurrent characters who have more than 90 dialogue lines in total, and then take a look at the list you have. If you've done this correctly, you should have a list of 20 names. However, one of these is clearly not a recurrent character. Manually remove that one, and print out your list of recurrent characters. To remove that character, pay attention to the _named character_ assumption we gave you earlier on. **For all the subsequent questions, you must only keep the dialogue lines said by the recurrent characters in your list.**\n",
    "\n",
    "_Hint: \"I know all the recurrent characters because I've watched the entire series five times\" is not an acceptable argument, so you need to actually generate the list._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "      <th>Scene</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Character</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amy</th>\n",
       "      <td>3219</td>\n",
       "      <td>187</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arthur</th>\n",
       "      <td>129</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernadette</th>\n",
       "      <td>2514</td>\n",
       "      <td>166</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bert</th>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beverley</th>\n",
       "      <td>159</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily</th>\n",
       "      <td>159</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Howard</th>\n",
       "      <td>5517</td>\n",
       "      <td>393</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kripke</th>\n",
       "      <td>106</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leonard</th>\n",
       "      <td>8770</td>\n",
       "      <td>534</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leslie</th>\n",
       "      <td>114</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Man</th>\n",
       "      <td>102</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs Cooper</th>\n",
       "      <td>212</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs Wolowitz</th>\n",
       "      <td>135</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Penny</th>\n",
       "      <td>6823</td>\n",
       "      <td>387</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Priya</th>\n",
       "      <td>215</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Raj</th>\n",
       "      <td>4546</td>\n",
       "      <td>341</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sheldon</th>\n",
       "      <td>11062</td>\n",
       "      <td>603</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stuart</th>\n",
       "      <td>701</td>\n",
       "      <td>61</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wil</th>\n",
       "      <td>124</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zack</th>\n",
       "      <td>129</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Line  Scene  Episode  Season\n",
       "Character                                  \n",
       "Amy            3219    187       24       8\n",
       "Arthur          129      9        3       3\n",
       "Bernadette     2514    166       24       8\n",
       "Bert             93     10        6       2\n",
       "Beverley        159     12        5       4\n",
       "Emily           159     24       14       4\n",
       "Howard         5517    393       24      10\n",
       "Kripke          106     16        8       5\n",
       "Leonard        8770    534       24      10\n",
       "Leslie          114     13        7       4\n",
       "Man             102     24       13       9\n",
       "Mrs Cooper      212     18        7       7\n",
       "Mrs Wolowitz    135     28       17       6\n",
       "Penny          6823    387       24      10\n",
       "Priya           215     29       12       2\n",
       "Raj            4546    341       24      10\n",
       "Sheldon       11062    603       24      10\n",
       "Stuart          701     61       22       9\n",
       "Wil             124     18       10       7\n",
       "Zack            129     15        6       5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Character').nunique()[(df.groupby('Character').nunique().Line>90) & (df.groupby('Character').nunique().Episode>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_characters = list(df.groupby('Character').nunique()[(df.groupby('Character').nunique().Line>90) & (df.groupby('Character').nunique().Episode>1)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amy',\n",
       " 'Arthur',\n",
       " 'Bernadette',\n",
       " 'Bert',\n",
       " 'Beverley',\n",
       " 'Emily',\n",
       " 'Howard',\n",
       " 'Kripke',\n",
       " 'Leonard',\n",
       " 'Leslie',\n",
       " 'Man',\n",
       " 'Mrs Cooper',\n",
       " 'Mrs Wolowitz',\n",
       " 'Penny',\n",
       " 'Priya',\n",
       " 'Raj',\n",
       " 'Sheldon',\n",
       " 'Stuart',\n",
       " 'Wil',\n",
       " 'Zack']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       48451\n",
       "unique         20\n",
       "top       Sheldon\n",
       "freq        11689\n",
       "Name: Character, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Character.isin(rec_characters)].Character.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "      <th>Character</th>\n",
       "      <th>Scene</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So if a photon is directed through a plane wit...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agreed, what’s your point?</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There’s no point, I just think it’s a good ide...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excuse me?</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>One across is Aegean, eight down is Nabakov, t...</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51284</th>\n",
       "      <td>Uh, breakfast yes, lunch no. I did have a coug...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51286</th>\n",
       "      <td>How thoughtful. Thank you.</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51288</th>\n",
       "      <td>And I with you. Question, are you seeking a ro...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51290</th>\n",
       "      <td>Well, that would raise a number of problems. W...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51291</th>\n",
       "      <td>(Knock, knock, knock) Amy. (Knock, knock, knoc...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Princeton.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48451 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Line Character  \\\n",
       "0      So if a photon is directed through a plane wit...   Sheldon   \n",
       "1                             Agreed, what’s your point?   Leonard   \n",
       "2      There’s no point, I just think it’s a good ide...   Sheldon   \n",
       "3                                             Excuse me?   Leonard   \n",
       "5      One across is Aegean, eight down is Nabakov, t...   Leonard   \n",
       "...                                                  ...       ...   \n",
       "51284  Uh, breakfast yes, lunch no. I did have a coug...   Sheldon   \n",
       "51286                         How thoughtful. Thank you.   Sheldon   \n",
       "51288  And I with you. Question, are you seeking a ro...   Sheldon   \n",
       "51290  Well, that would raise a number of problems. W...   Sheldon   \n",
       "51291  (Knock, knock, knock) Amy. (Knock, knock, knoc...   Sheldon   \n",
       "\n",
       "                             Scene  Episode  Season  \n",
       "0      A corridor at a sperm bank.        1       1  \n",
       "1      A corridor at a sperm bank.        1       1  \n",
       "2      A corridor at a sperm bank.        1       1  \n",
       "3      A corridor at a sperm bank.        1       1  \n",
       "5      A corridor at a sperm bank.        1       1  \n",
       "...                            ...      ...     ...  \n",
       "51284            Sheldon’s office.       24      10  \n",
       "51286            Sheldon’s office.       24      10  \n",
       "51288            Sheldon’s office.       24      10  \n",
       "51290            Sheldon’s office.       24      10  \n",
       "51291                   Princeton.       24      10  \n",
       "\n",
       "[48451 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.Character.isin(rec_characters)]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B: Read the ~~stats~~ scripts carefully (30 points)\n",
    "\n",
    "### Part 1: Don't put the shovel down just yet\n",
    "\n",
    "**Q3**. (2.5 points) From each dialogue line, replace punctuation marks (listed in the EXCLUDE_CHARS variable provided in `helpers/helper_functions.py`) with whitespaces, and lowercase all the text. **Do not remove any stopwords, leave them be for all the questions in this task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rm/7_ymgfb97mbb78p4rhq92vww0000gn/T/ipykernel_80982/2017540973.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.Line = df.Line.apply(lambda line: replace_more(EXCLUDE_CHARS,line).lower())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "      <th>Character</th>\n",
       "      <th>Scene</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so if a photon is directed through a plane wit...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agreed  what s your point</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there s no point  i just think it s a good ide...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excuse me</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>one across is aegean  eight down is nabakov  t...</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51284</th>\n",
       "      <td>uh  breakfast yes  lunch no  i did have a coug...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51286</th>\n",
       "      <td>how thoughtful  thank you</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51288</th>\n",
       "      <td>and i with you  question  are you seeking a ro...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51290</th>\n",
       "      <td>well  that would raise a number of problems  w...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51291</th>\n",
       "      <td>knock  knock  knock  amy   knock  knock  knoc...</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Princeton.</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48451 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Line Character  \\\n",
       "0      so if a photon is directed through a plane wit...   Sheldon   \n",
       "1                             agreed  what s your point    Leonard   \n",
       "2      there s no point  i just think it s a good ide...   Sheldon   \n",
       "3                                             excuse me    Leonard   \n",
       "5      one across is aegean  eight down is nabakov  t...   Leonard   \n",
       "...                                                  ...       ...   \n",
       "51284  uh  breakfast yes  lunch no  i did have a coug...   Sheldon   \n",
       "51286                         how thoughtful  thank you    Sheldon   \n",
       "51288  and i with you  question  are you seeking a ro...   Sheldon   \n",
       "51290  well  that would raise a number of problems  w...   Sheldon   \n",
       "51291   knock  knock  knock  amy   knock  knock  knoc...   Sheldon   \n",
       "\n",
       "                             Scene  Episode  Season  \n",
       "0      A corridor at a sperm bank.        1       1  \n",
       "1      A corridor at a sperm bank.        1       1  \n",
       "2      A corridor at a sperm bank.        1       1  \n",
       "3      A corridor at a sperm bank.        1       1  \n",
       "5      A corridor at a sperm bank.        1       1  \n",
       "...                            ...      ...     ...  \n",
       "51284            Sheldon’s office.       24      10  \n",
       "51286            Sheldon’s office.       24      10  \n",
       "51288            Sheldon’s office.       24      10  \n",
       "51290            Sheldon’s office.       24      10  \n",
       "51291                   Princeton.       24      10  \n",
       "\n",
       "[48451 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from string import punctuation\n",
    "\n",
    "# EXCLUDE_CHARS = set(punctuation).union(set('’'))\n",
    "\n",
    "def replace_more(chars,string):\n",
    "    for char in chars:\n",
    "        string = string.replace(char,' ')\n",
    "    return string\n",
    "\n",
    "df.Line = df.Line.apply(lambda line: replace_more(EXCLUDE_CHARS,line).lower())\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**. (5 points) For each term, calculate its \"corpus frequency\", i.e. its number of occurrences in the entire series. Visualize the distribution of corpus frequency using a histogram. Explain your observations. What are the appropriate x and y scales for this plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['younger', 'your', 'yours', 'yourself', 'yourselves', 'youth',\n",
       "       'youtube', 'yum', 'yummy', 'yup', 'zachary', 'zack', 'zarnecki',\n",
       "       'zero', 'zinger', 'zip', 'zombie', 'zombies', 'zone', 'zoo'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn, Count vectorizer for bag of words features\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.4)\n",
    "\n",
    "#initialize and specify minumum number of occurences to avoid untractable number of features\n",
    "#vectorizer = CountVectorizer(min_df = 2) if we want high frequency\n",
    "\n",
    "#create bag of words features\n",
    "X = vectorizer.fit_transform(df.Line)\n",
    "vectorizer.get_feature_names_out().T[-20:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48451, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5632,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Corpus frequency\n",
    "X.toarray().sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5632,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5632)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([X.toarray().sum(axis=0),vectorizer.get_feature_names_out()]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5627</th>\n",
       "      <td>14</td>\n",
       "      <td>zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5628</th>\n",
       "      <td>11</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5629</th>\n",
       "      <td>12</td>\n",
       "      <td>zombies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5630</th>\n",
       "      <td>30</td>\n",
       "      <td>zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631</th>\n",
       "      <td>14</td>\n",
       "      <td>zoo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5632 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     freq     word\n",
       "0       5       00\n",
       "1      33      000\n",
       "2       8       10\n",
       "3       9      100\n",
       "4      23       11\n",
       "...   ...      ...\n",
       "5627   14      zip\n",
       "5628   11   zombie\n",
       "5629   12  zombies\n",
       "5630   30     zone\n",
       "5631   14      zoo\n",
       "\n",
       "[5632 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get corpus frequency of each word\n",
    "corpus_freq = pd.DataFrame(np.array([X.toarray().sum(axis=0),vectorizer.get_feature_names_out()]).T,columns=['freq','word'])\n",
    "corpus_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x='freq',y='word',data=corpus_freq.sort_values('freq',ascending=False))\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Talkativity\n",
    "**Q5**. (2.5 points) For each of the recurrent characters, calculate their total number of words uttered across all episodes. Based on this, who seems to be the most talkative character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('asdas asda sd sd as dasd as asd'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592210"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total words said\n",
    "df.Line.str.split().str.len().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Character\n",
       "Amy              39933\n",
       "Arthur            1451\n",
       "Bernadette       27726\n",
       "Bert              1146\n",
       "Beverley          2029\n",
       "Emily             1571\n",
       "Howard           69505\n",
       "Kripke            1246\n",
       "Leonard         102496\n",
       "Leslie            1249\n",
       "Man               1253\n",
       "Mrs Cooper        3389\n",
       "Mrs Wolowitz      1459\n",
       "Penny            79270\n",
       "Priya             1940\n",
       "Raj              60099\n",
       "Sheldon         185388\n",
       "Stuart            7955\n",
       "Wil               1678\n",
       "Zack              1427\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_words(char_df):\n",
    "    return char_df.Line.str.split().str.len().sum()\n",
    "\n",
    "# words said by character \n",
    "df.groupby('Character').apply(count_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6**. (12.5 points) For each of the recurrent characters, calculate their total number of words uttered per episode (ignoring episodes that the character does not appear in), and calculate a **robust summary statistic** for the word count distribution of each person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Character  Season  Episode\n",
       "Amy        3       23         103\n",
       "           4       1          143\n",
       "                   3          473\n",
       "                   5          229\n",
       "                   8          602\n",
       "                             ... \n",
       "Zack       4       17         153\n",
       "           7       9          126\n",
       "                   11          17\n",
       "           9       22         218\n",
       "           10      22         282\n",
       "Length: 1663, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Character','Season','Episode']).apply(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Character  Season  Episode\n",
       "Amy        3       23         103\n",
       "           4       1          143\n",
       "                   3          473\n",
       "                   5          229\n",
       "                   8          602\n",
       "                             ... \n",
       "Zack       4       17         153\n",
       "           7       9          126\n",
       "                   11          17\n",
       "           9       22         218\n",
       "           10      22         282\n",
       "Length: 1663, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Character','Season','Episode']).apply(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Character\n",
       "Amy             234.0\n",
       "Arthur          325.0\n",
       "Bernadette      157.0\n",
       "Bert            199.0\n",
       "Beverley        184.0\n",
       "Emily            87.0\n",
       "Howard          289.0\n",
       "Kripke          162.0\n",
       "Leonard         397.0\n",
       "Leslie          121.0\n",
       "Man              11.0\n",
       "Mrs Cooper      293.5\n",
       "Mrs Wolowitz     39.0\n",
       "Penny           323.0\n",
       "Priya           145.0\n",
       "Raj             219.0\n",
       "Sheldon         775.0\n",
       "Stuart          112.5\n",
       "Wil             122.5\n",
       "Zack            139.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# median words per episode of each character\n",
    "df.groupby(['Character','Season','Episode']).apply(count_words).groupby('Character').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Character</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amy</th>\n",
       "      <td>234.0</td>\n",
       "      <td>39933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arthur</th>\n",
       "      <td>325.0</td>\n",
       "      <td>1451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernadette</th>\n",
       "      <td>157.0</td>\n",
       "      <td>27726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bert</th>\n",
       "      <td>199.0</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beverley</th>\n",
       "      <td>184.0</td>\n",
       "      <td>2029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily</th>\n",
       "      <td>87.0</td>\n",
       "      <td>1571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Howard</th>\n",
       "      <td>289.0</td>\n",
       "      <td>69505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kripke</th>\n",
       "      <td>162.0</td>\n",
       "      <td>1246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leonard</th>\n",
       "      <td>397.0</td>\n",
       "      <td>102496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leslie</th>\n",
       "      <td>121.0</td>\n",
       "      <td>1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Man</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs Cooper</th>\n",
       "      <td>293.5</td>\n",
       "      <td>3389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs Wolowitz</th>\n",
       "      <td>39.0</td>\n",
       "      <td>1459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Penny</th>\n",
       "      <td>323.0</td>\n",
       "      <td>79270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Priya</th>\n",
       "      <td>145.0</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Raj</th>\n",
       "      <td>219.0</td>\n",
       "      <td>60099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sheldon</th>\n",
       "      <td>775.0</td>\n",
       "      <td>185388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stuart</th>\n",
       "      <td>112.5</td>\n",
       "      <td>7955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wil</th>\n",
       "      <td>122.5</td>\n",
       "      <td>1678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zack</th>\n",
       "      <td>139.5</td>\n",
       "      <td>1427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0       1\n",
       "Character                  \n",
       "Amy           234.0   39933\n",
       "Arthur        325.0    1451\n",
       "Bernadette    157.0   27726\n",
       "Bert          199.0    1146\n",
       "Beverley      184.0    2029\n",
       "Emily          87.0    1571\n",
       "Howard        289.0   69505\n",
       "Kripke        162.0    1246\n",
       "Leonard       397.0  102496\n",
       "Leslie        121.0    1249\n",
       "Man            11.0    1253\n",
       "Mrs Cooper    293.5    3389\n",
       "Mrs Wolowitz   39.0    1459\n",
       "Penny         323.0   79270\n",
       "Priya         145.0    1940\n",
       "Raj           219.0   60099\n",
       "Sheldon       775.0  185388\n",
       "Stuart        112.5    7955\n",
       "Wil           122.5    1678\n",
       "Zack          139.5    1427"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df.groupby(['Character','Season','Episode']).apply(count_words).groupby('Character').median(), \n",
    "           df.groupby('Character').apply(count_words)],axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**i)** (2.5 points) What changes do you observe, compared to the analysis in Q5?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That some characters like Amy speak a lot because they appear in a lot of episodes, rather because they spoke a lot per episode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**ii)** (2.5 points) Why is this analysis an improvement over the previous one, and how could you improve it even further? _Hint: The improvement involves making your unit for word counts even more granular - you can go further down than episodes._\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**iii)** (7.5 points) Incorporate that improvement. Do you still see the same results? How **confident** can you be that the \"most talkative\" person given by this twice improved method is really more talkative than the second most talkative one? _Hint: Read the question again. A good idea would be to use bootstrapping and calculate your summary statistic on each bootstrapped set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Character\n",
       "Amy              65.5\n",
       "Arthur          107.5\n",
       "Bernadette       52.0\n",
       "Bert             82.0\n",
       "Beverley        107.0\n",
       "Emily            34.5\n",
       "Howard           64.0\n",
       "Kripke           57.5\n",
       "Leonard          67.0\n",
       "Leslie           61.0\n",
       "Man              13.0\n",
       "Mrs Cooper      128.5\n",
       "Mrs Wolowitz     26.0\n",
       "Penny            71.0\n",
       "Priya            44.0\n",
       "Raj              59.0\n",
       "Sheldon         117.0\n",
       "Stuart           52.5\n",
       "Wil              46.5\n",
       "Zack             61.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# median words per episode of each character\n",
    "df.groupby(['Character', 'Season', 'Episode', 'Scene']\n",
    "           ).apply(count_words\n",
    "                   ).groupby('Character').median()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's simpson's paradox: aggregating dat can lead you to false conclusions about something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>per scene</th>\n",
       "      <th>per episode</th>\n",
       "      <th>total words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Character</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amy</th>\n",
       "      <td>65.5</td>\n",
       "      <td>234.0</td>\n",
       "      <td>39933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arthur</th>\n",
       "      <td>107.5</td>\n",
       "      <td>325.0</td>\n",
       "      <td>1451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernadette</th>\n",
       "      <td>52.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>27726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bert</th>\n",
       "      <td>82.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beverley</th>\n",
       "      <td>107.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>2029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily</th>\n",
       "      <td>34.5</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Howard</th>\n",
       "      <td>64.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>69505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kripke</th>\n",
       "      <td>57.5</td>\n",
       "      <td>162.0</td>\n",
       "      <td>1246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leonard</th>\n",
       "      <td>67.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>102496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leslie</th>\n",
       "      <td>61.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Man</th>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs Cooper</th>\n",
       "      <td>128.5</td>\n",
       "      <td>293.5</td>\n",
       "      <td>3389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs Wolowitz</th>\n",
       "      <td>26.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Penny</th>\n",
       "      <td>71.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>79270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Priya</th>\n",
       "      <td>44.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Raj</th>\n",
       "      <td>59.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>60099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sheldon</th>\n",
       "      <td>117.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>185388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stuart</th>\n",
       "      <td>52.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>7955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wil</th>\n",
       "      <td>46.5</td>\n",
       "      <td>122.5</td>\n",
       "      <td>1678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zack</th>\n",
       "      <td>61.0</td>\n",
       "      <td>139.5</td>\n",
       "      <td>1427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              per scene  per episode  total words\n",
       "Character                                        \n",
       "Amy                65.5        234.0        39933\n",
       "Arthur            107.5        325.0         1451\n",
       "Bernadette         52.0        157.0        27726\n",
       "Bert               82.0        199.0         1146\n",
       "Beverley          107.0        184.0         2029\n",
       "Emily              34.5         87.0         1571\n",
       "Howard             64.0        289.0        69505\n",
       "Kripke             57.5        162.0         1246\n",
       "Leonard            67.0        397.0       102496\n",
       "Leslie             61.0        121.0         1249\n",
       "Man                13.0         11.0         1253\n",
       "Mrs Cooper        128.5        293.5         3389\n",
       "Mrs Wolowitz       26.0         39.0         1459\n",
       "Penny              71.0        323.0        79270\n",
       "Priya              44.0        145.0         1940\n",
       "Raj                59.0        219.0        60099\n",
       "Sheldon           117.0        775.0       185388\n",
       "Stuart             52.5        112.5         7955\n",
       "Wil                46.5        122.5         1678\n",
       "Zack               61.0        139.5         1427"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([  # median words per episode of each character\n",
    "    df.groupby(['Character', 'Season', 'Episode', 'Scene']\n",
    "               ).apply(count_words\n",
    "                       ).groupby('Character').median(),\n",
    "    df.groupby(['Character', 'Season', 'Episode']).apply(\n",
    "        count_words).groupby('Character').median(),\n",
    "    df.groupby('Character').apply(count_words)], axis=1).rename(columns={0:'per scene',1:'per episode',2:'total words'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 3: Obligatory Spark cameo\n",
    "**Q7**. (7.5 points) Write a Spark script that does the following: Given the raw input file and your list of recurrent characters, create an RDD containing (speaker, dialogue line) rows **only for the recurrent characters** (assume that you already have the list --  no need to calculate it using Spark), and then generate a vectorized bag of words representation for each dialogue line, thus generating an RDD with (speaker, bag of words vector) rows. Then, calculate an aggregated bag of words vector (sum of all vectors) for each person. The final output is therefore an RDD with each of its rows being (speaker, aggregated bag of words vector). For your bag of words vectors, you can use $1\\times|V|$ scipy CSR matrices (where $|V|$ is the size of the vocabulary). No filtering of the vocabulary is necessary for this part.\n",
    "\n",
    "You do not need to run this script, but you do need to use Spark logic and also, the syntax needs to be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task C: The Gossip Graph (30 points)\n",
    "\n",
    "**Note: Only for this task, discard the recurrent characters whose names are not single words, e.g. Mrs. Cooper.**\n",
    "\n",
    "Let us define _gossip_ as follows: if a dialogue line of character A mentions B by name in a scene that does not involve character B, we say that “A gossips about B” in that line. Multiple mentions of the same person in a single line are counted once, but a character can gossip about several others in the same line. For the sake of simplicity, we only consider gossips where the name of the recurrent character is mentioned as it appears in our list of characters; for example, if someone says \"Cooper\" and they mean Sheldon, we discard that.\n",
    "\n",
    "**Q8**. (12.5 points) Create the two following graphs first:\n",
    "\n",
    "1. (5 points) Create the _familiarity graph_, an undirected weighted graph, in which there is a node for each recurrent character, and an edge between two characters if they appear together in at least one scene. The weight of the edge between them is the number of scenes they appear in together. If an edge exists between two people in the familiarity graph, we say that they \"know each other\".\n",
    "2. (7.5 points) Create the _gossip graph_, which is a directed weighted graph, in which there there is a node for each recurrent character, and a directed edge from the node for A to the node for B if A has gossiped about B at least once. The weight of the edge is the number of scenes in which A has gossiped about B.\n",
    "\n",
    "_Hint: You can create each graph first as an adjacency matrix and then create a networkx graph out of that._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, answer the following questions:\n",
    "\n",
    "**Q9**. (5 points) Sheldon claims that every character in the show is familiar with everyone else through at most one intermediary. Based on the familiarity graph, is this true? If not, at most how many intermediaries are needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10**. (5 points) Who is the character through whom the largest number of these indirect familiarities happen? Calculate an appropriate centrality metric on the familiarity graph to answer this question. You can use the package networkx for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11**. (2.5 points) Another claim of Sheldon's is that every recurrent character in the show gossips about all the other recurrent characters. What property of the gossip graph would correspond to this? Does the gossip graph possess that property? If not, then is it the case that for every pair of recurrent characters, at least one gossips about the other? What property would this correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12**. (5 points) Use the gossip graph and the familiarity graph to figure out if for every pair of recurrent characters, one of them has gossiped about the other if and only if they know each other. Explain your method - the simpler, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task D: The Detective's Hat (30 points)\n",
    "\n",
    "Sheldon claims that given a dialogue line, he can, with an accuracy of above 70%, say whether it's by himself or by someone else. Leonard contests this claim, since he believes that this claimed accuracy is too high. Leonard also suspects that it's easier for Sheldon to distinguish the lines that _aren't_ his, rather than those that _are_. We want you to put on the (proverbial) detective's hat and to investigate this claim.\n",
    "\n",
    "**Q13**. (7.5 points) Divide the set of all dialogue lines into two subsets: the training set, consisting of all the seasons except the last two, and the test set, consisting of the last two seasons. Each of your data points (which is one row of your matrix) is one **dialogue line**. Now, use the scikit-learn class **TfIdfVectorizer** to create TF-IDF representations for the data points in your training and test sets. Note that since you're going to train a machine learning model, everything used in the training needs to be independent of the test set. As a preprocessing step, remove stopwords and words that appear only once from your vocabulary. Use the simple tokenizer provided in `helpers/helper_functions.py` as an input to the TfidfVectorizer class, and use the words provided in `helpers/stopwords.txt` as your stopwords."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preprocessing step, remove stopwords and words that appear only once from your vocabulary. Use the simple tokenizer provided in `helpers/helper_functions.py` as an input to the TfidfVectorizer class, and use the words provided in `helpers/stopwords.txt` as your stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = [stp.strip() for stp in open('helpers/stopwords.txt','r').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rm/7_ymgfb97mbb78p4rhq92vww0000gn/T/ipykernel_80982/3797994344.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.Line = df.Line.apply(simple_tokeniser).apply(remove_stopwords).apply(lambda tokens: ' '.join(tokens))\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in STOPWORDS]\n",
    "df.Line = df.Line.apply(simple_tokeniser).apply(remove_stopwords).apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='Season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = df[~df.Season.isin([9, 10])], df[df.Season.isin(\n",
    "    [9, 10])], df.Character[~df.Season.isin([9, 10])], df.Character[df.Season.isin([9, 10])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '000lb', ..., 'आज', 'घर', 'नय'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(df.Line)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
    "X_train_tf = vectorizer.fit_transform(X_train.Line)\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
    "X_test_tf = vectorizer.fit_transform(X_test.Line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(38244, 20451), (10207, 20451), (38244,), (10207,)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[X_train_tf.toarray().shape,X_test_tf.toarray().shape, y_train.shape, y_test.shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14**. (5 points) Find the set of all words in the training set that are only uttered by Sheldon. Is it possible for Sheldon to identify himself only based on these? Use the test set to assess this possibility, and explain your method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    0,     1,     2,     4,     6,     8,    10,    12,    14,\n",
       "               16,\n",
       "            ...\n",
       "            38205, 38221, 38226, 38228, 38232, 38234, 38235, 38236, 38240,\n",
       "            38241],\n",
       "           dtype='int64', length=9452)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.reset_index(drop=True).loc[X_train.reset_index(drop=True).Character=='Sheldon'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9452, 20451)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf[X_train.reset_index(drop=True).loc[X_train.reset_index(drop=True).Character=='Sheldon'].index].toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_not_by_sheldon = X_train[X_train.Character!='Sheldon'].Line.str.cat(sep = ' ').split()\n",
    "words_by_sheldon = [word for word in \n",
    "                        X_train[X_train.Character=='Sheldon'].Line.str.cat(sep = ' ').split()\n",
    "                        if word not in words_not_by_sheldon\n",
    "                    ]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7141"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_by_sheldon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slits',\n",
       " 'slits',\n",
       " 'unobserved',\n",
       " 'slits',\n",
       " 'teleportation',\n",
       " 'transmit',\n",
       " 'reassembly',\n",
       " 'transported',\n",
       " 'recreated',\n",
       " 'disintegrated',\n",
       " 'baster',\n",
       " 'labelled',\n",
       " 'mucus',\n",
       " 'congested',\n",
       " 'keen',\n",
       " 'arrival',\n",
       " 'rendered',\n",
       " 'collaborate',\n",
       " 'schematic',\n",
       " 'argon',\n",
       " 'hunches',\n",
       " 'guesses',\n",
       " 'first…',\n",
       " 'labourers',\n",
       " 'vision',\n",
       " 'oompah',\n",
       " 'loompahs',\n",
       " 'invalidated',\n",
       " 'pubescent',\n",
       " 'wunderkind',\n",
       " 'ceases',\n",
       " 'fruitless',\n",
       " 'efforts',\n",
       " 'donates',\n",
       " 'waits',\n",
       " 'asterisk',\n",
       " 'cyborg',\n",
       " 'excelled',\n",
       " 'wolfgang',\n",
       " 'amadeus',\n",
       " 'mozart',\n",
       " 'moss',\n",
       " 'frankfurter',\n",
       " 'croutons',\n",
       " 'pea',\n",
       " 'frankfurter',\n",
       " 'croutons',\n",
       " 'carrier',\n",
       " '187',\n",
       " 'febrile',\n",
       " 'delirium',\n",
       " 'heidelberg',\n",
       " 'germany',\n",
       " 'visiting',\n",
       " 'blitzkrieg',\n",
       " 'intestine',\n",
       " 'czechoslovakia',\n",
       " 'clockwise',\n",
       " 'mats',\n",
       " 'cinderblocks',\n",
       " 'housekeeper',\n",
       " 'residence',\n",
       " '“möchtest',\n",
       " 'darmspülung',\n",
       " '“would',\n",
       " 'adoseconds',\n",
       " 'resistance',\n",
       " 'teamwork',\n",
       " 'anthropology',\n",
       " 'mammal',\n",
       " 'manners',\n",
       " 'salted',\n",
       " 'carthage',\n",
       " 'tawdry',\n",
       " 'picasso',\n",
       " 'noah',\n",
       " 'webster',\n",
       " 'jacques',\n",
       " 'cousteau',\n",
       " 'chuppah',\n",
       " 'bobsled',\n",
       " 'traditionally',\n",
       " 'fierce',\n",
       " 'creatures',\n",
       " 'opponent',\n",
       " 'gram',\n",
       " 'gram',\n",
       " 'exceeds',\n",
       " 'unanimous',\n",
       " 'emblazon',\n",
       " 'particulate',\n",
       " 'chaos',\n",
       " 'brace',\n",
       " 'anodised',\n",
       " 'decade',\n",
       " 'disappeared',\n",
       " 'refocus',\n",
       " 'efforts',\n",
       " 'jerusalem',\n",
       " 'oracle',\n",
       " 'neo',\n",
       " 'hava',\n",
       " 'nagila',\n",
       " 'prize…',\n",
       " 'goldfarb',\n",
       " 'senoran',\n",
       " 'nuevo',\n",
       " 'cherusalem',\n",
       " 'pharoah',\n",
       " 'consideration',\n",
       " 'nanotubes',\n",
       " 'tensile',\n",
       " 'bubbula',\n",
       " 'cogent',\n",
       " 'nanotubes',\n",
       " 'causal',\n",
       " 'curiouser',\n",
       " 'curiouser',\n",
       " 'vulcans',\n",
       " 'wailing',\n",
       " 'jerusalem',\n",
       " 'wail',\n",
       " 'youth',\n",
       " 'naivety',\n",
       " 'inexplicable',\n",
       " 'assure',\n",
       " 'uninterrupted',\n",
       " 'baffle',\n",
       " 'repulse',\n",
       " 'cyborgs',\n",
       " 'pleasing',\n",
       " 'egotist',\n",
       " 'downhill',\n",
       " 'gasses',\n",
       " 'apparent',\n",
       " 'explained',\n",
       " 'repeatedly',\n",
       " 'flamingo',\n",
       " 'ritalin',\n",
       " 'physiological',\n",
       " 'characteristic',\n",
       " 'cycle',\n",
       " 'relevant',\n",
       " 'factor',\n",
       " 'reprogrammed',\n",
       " 'petite',\n",
       " 'intelligences',\n",
       " 'teen',\n",
       " 'fetishes',\n",
       " 'rendering',\n",
       " 'moot',\n",
       " 'pkshhhh',\n",
       " 'marriott',\n",
       " 'judgemental',\n",
       " 'efficient',\n",
       " 'tassles',\n",
       " 'nourishment',\n",
       " 'expel',\n",
       " 'inhale',\n",
       " 'libido',\n",
       " 'pandering',\n",
       " 'undignified',\n",
       " 'preferable',\n",
       " 'misunder…',\n",
       " 'physiological',\n",
       " 'manifestations',\n",
       " 'turmoil',\n",
       " 'awkwardness',\n",
       " 'vocation',\n",
       " 'avocation',\n",
       " 'addict',\n",
       " 'candyland',\n",
       " 'humbling',\n",
       " 'certainty',\n",
       " 'loobenfeld',\n",
       " 'plausible',\n",
       " 'predisposition',\n",
       " 'inadequate',\n",
       " 'subtextually',\n",
       " 'predisposition',\n",
       " 'influenza',\n",
       " 'runny',\n",
       " 'exponentially',\n",
       " '2am',\n",
       " 'producing',\n",
       " 'sputum',\n",
       " 'gasses',\n",
       " 'ionised',\n",
       " 'bends',\n",
       " 'consideration',\n",
       " 'petrie',\n",
       " 'apricot',\n",
       " 'swab',\n",
       " 'pathogen',\n",
       " 'typhoid',\n",
       " 'cornhusking',\n",
       " 'antibodies',\n",
       " 'comatose',\n",
       " 'relying',\n",
       " 'inferior',\n",
       " 'accompanying',\n",
       " 'jug',\n",
       " 'output',\n",
       " 'quintessential',\n",
       " 'rooted',\n",
       " 'indigestion',\n",
       " 'laughably',\n",
       " 'transparent',\n",
       " 'exquisitely',\n",
       " 'convoluted',\n",
       " 'googles',\n",
       " 'wrung',\n",
       " 'relapse',\n",
       " 'invites',\n",
       " 'addled',\n",
       " 'analysing',\n",
       " 'www',\n",
       " 'socalphysicsgroup',\n",
       " 'org',\n",
       " 'scroll',\n",
       " 'seminars',\n",
       " 'bippidy',\n",
       " 'boppidy',\n",
       " 'igniting',\n",
       " 'docked',\n",
       " 'liner',\n",
       " 'surprisingly',\n",
       " 'gripping',\n",
       " 'biochemical',\n",
       " 'realised',\n",
       " 'quintessential',\n",
       " 'rebelling',\n",
       " 'destruction',\n",
       " 'hesitated',\n",
       " 'predicament',\n",
       " 'inadequate',\n",
       " 'deceit',\n",
       " 'manages',\n",
       " 'emile',\n",
       " 'geometry',\n",
       " 'burner',\n",
       " '802',\n",
       " '11n',\n",
       " 'router',\n",
       " 'ethernet',\n",
       " 'ports',\n",
       " '640',\n",
       " 'burners',\n",
       " 'burner',\n",
       " 'krob',\n",
       " 'lottery',\n",
       " 'scratchers',\n",
       " 'hack',\n",
       " 'twelfth',\n",
       " 'motorised',\n",
       " 'motorised',\n",
       " 'toll',\n",
       " 'taker',\n",
       " 'rant',\n",
       " 'concise',\n",
       " 'summation',\n",
       " 'bertram',\n",
       " 'forer',\n",
       " '1948',\n",
       " 'pseudo',\n",
       " 'refocused',\n",
       " 'bosonic',\n",
       " 'heteronic',\n",
       " 'weep',\n",
       " 'verify',\n",
       " 'conspirators',\n",
       " 'unverified',\n",
       " 'frivolity',\n",
       " 'laborious',\n",
       " 'simplify',\n",
       " 'richer',\n",
       " 'denied',\n",
       " 'endure',\n",
       " 'conical',\n",
       " 'castles',\n",
       " 'spun',\n",
       " 'grotesque',\n",
       " 'tailless',\n",
       " 'mocked',\n",
       " 'disorientation',\n",
       " 'overlapping',\n",
       " 'endeavour',\n",
       " 'familiarity',\n",
       " 'excess',\n",
       " 'epithelial',\n",
       " 'slough',\n",
       " 'accelerate',\n",
       " 'amounts',\n",
       " 'producing',\n",
       " 'concentration',\n",
       " 'fantasia',\n",
       " 'densities',\n",
       " 'patterns',\n",
       " 'dispersion',\n",
       " 'sunlight',\n",
       " 'menelaus',\n",
       " 'helen',\n",
       " 'troy',\n",
       " 'menelaus',\n",
       " 'agamemnon…',\n",
       " 'clockwise',\n",
       " 'aieee',\n",
       " 'xia',\n",
       " 'si',\n",
       " 'citrus',\n",
       " 'peels',\n",
       " 'gei',\n",
       " 'kan',\n",
       " 'jud',\n",
       " 'pei',\n",
       " 'citrus',\n",
       " 'peels',\n",
       " 'gei',\n",
       " 'kan',\n",
       " 'jud',\n",
       " 'pei',\n",
       " 'glare',\n",
       " 'absorbs',\n",
       " 'reduces',\n",
       " 'expulsion',\n",
       " 'grammatical',\n",
       " 'nauseated',\n",
       " 'improbable',\n",
       " 'conclude',\n",
       " 'progeny',\n",
       " 'keepers',\n",
       " 'scooby',\n",
       " 'cartoons',\n",
       " 'hou',\n",
       " 'shui',\n",
       " 'zai',\n",
       " 'mucus',\n",
       " 'mucus',\n",
       " 'mucus',\n",
       " 'duration',\n",
       " 'grossinger',\n",
       " 'hairless',\n",
       " 'yip…',\n",
       " 'diverted',\n",
       " '1935',\n",
       " 'erwin',\n",
       " 'sealed',\n",
       " '1935',\n",
       " 'erwin',\n",
       " 'schrodinger…',\n",
       " 'sha',\n",
       " 'pwe',\n",
       " 'caption',\n",
       " '“long',\n",
       " 'concrete”',\n",
       " 'xie',\n",
       " 'xie',\n",
       " 'mashed',\n",
       " 'redundancy',\n",
       " 'narrow',\n",
       " 'mai',\n",
       " 'lui',\n",
       " 'primitive',\n",
       " 'neocortex',\n",
       " 'overpower',\n",
       " 'latter',\n",
       " 'credence',\n",
       " 'rendered',\n",
       " 'speechless',\n",
       " 'intervene',\n",
       " 'zhing',\n",
       " 'renditions',\n",
       " 'compelling',\n",
       " 'twelfth',\n",
       " 'chivalry',\n",
       " 'knighted',\n",
       " 'vertigo',\n",
       " 'vertigo',\n",
       " 'humorous',\n",
       " 'apocalyptic',\n",
       " 'splintered',\n",
       " 'factions',\n",
       " 'terranean',\n",
       " 'feasting',\n",
       " 'dwelling',\n",
       " 'eloy',\n",
       " 'practicality',\n",
       " 'memorabilia',\n",
       " 'venn',\n",
       " 'intersection',\n",
       " 'machine”',\n",
       " '“need',\n",
       " '800”',\n",
       " 'irritated',\n",
       " 'motivating',\n",
       " 'demeanour',\n",
       " 'irritability…',\n",
       " 'obtain',\n",
       " 'stardate',\n",
       " '5027',\n",
       " 'reckoning',\n",
       " 'formal',\n",
       " 'protest',\n",
       " 'velcro',\n",
       " 'brace',\n",
       " '1855',\n",
       " 'ergs',\n",
       " 'lamda',\n",
       " 'sigma',\n",
       " 'arrangement',\n",
       " 'spanish',\n",
       " 'recruited',\n",
       " 'ss',\n",
       " 'sinking',\n",
       " 'polymerised',\n",
       " 'sap',\n",
       " 'adhesive',\n",
       " 'projectile',\n",
       " 'trajectory',\n",
       " 'adheres',\n",
       " 'predates',\n",
       " 'acquisition',\n",
       " 'democracy',\n",
       " 'fist',\n",
       " 'fist',\n",
       " 'formal',\n",
       " 'protest',\n",
       " 'unmitigated',\n",
       " 'roast',\n",
       " 'wholewheat',\n",
       " 'roast',\n",
       " 'wholewheat',\n",
       " 'proper',\n",
       " 'formal',\n",
       " 'protest',\n",
       " 'informal',\n",
       " 'protest',\n",
       " 'decline',\n",
       " 'forfeited',\n",
       " 'immature',\n",
       " 'ceramic',\n",
       " 'insistence',\n",
       " 'curriculum',\n",
       " 'rendered',\n",
       " 'quizzical',\n",
       " 'fraternal',\n",
       " 'siblings',\n",
       " 'interject',\n",
       " 'ordering',\n",
       " 'pepperoni',\n",
       " 'cheeseless',\n",
       " 'niece',\n",
       " 'eskimo',\n",
       " 'truthfully',\n",
       " 'mutation',\n",
       " 'existing',\n",
       " 'mediocre',\n",
       " 'residing',\n",
       " 'freckling',\n",
       " 'decider',\n",
       " 'likelihood',\n",
       " 'frequent',\n",
       " 'dramatically',\n",
       " 'fertiliziation',\n",
       " 'hatched',\n",
       " 'siblings',\n",
       " 'donor',\n",
       " 'fertilized',\n",
       " 'implanted',\n",
       " 'correction',\n",
       " 'deems',\n",
       " 'noteworth',\n",
       " 'heavier',\n",
       " 'affluence',\n",
       " 'noteworthy',\n",
       " 'criticise',\n",
       " 'detour',\n",
       " 'fallopian',\n",
       " 'squabbling',\n",
       " '165',\n",
       " 'spidey',\n",
       " 'flawed',\n",
       " 'mimeaux',\n",
       " 'mcfly',\n",
       " 'rushes',\n",
       " 'nonchalant',\n",
       " 'humourmometer',\n",
       " '“grown',\n",
       " 'toys”',\n",
       " 'beanie',\n",
       " 'accumulator',\n",
       " 'ponies',\n",
       " 'frolicking',\n",
       " 'creatures',\n",
       " 'menacingly',\n",
       " 'elp',\n",
       " 'aaaaargh',\n",
       " 'asimov',\n",
       " 'emo',\n",
       " 'compiling',\n",
       " 'wreckage',\n",
       " 'analyse',\n",
       " 'dietician',\n",
       " 'limp',\n",
       " 'signalling',\n",
       " 'availability',\n",
       " 'dense',\n",
       " 'aramis',\n",
       " 'dissipate',\n",
       " 'rugby',\n",
       " 'awkwardly',\n",
       " 'crescent',\n",
       " 'patterns',\n",
       " 'conceal',\n",
       " 'feedback',\n",
       " 'manipulation',\n",
       " 'sparsely',\n",
       " 'sourced',\n",
       " 'infatuation',\n",
       " 'devolving',\n",
       " 'infra',\n",
       " 'repeater',\n",
       " 'photocell',\n",
       " 'emitter',\n",
       " 'objection',\n",
       " 'solely',\n",
       " 'imposition',\n",
       " 'organisational',\n",
       " 'disbelief',\n",
       " 'responsive',\n",
       " 'wooh',\n",
       " 'thorough',\n",
       " 'peasants',\n",
       " 'descriptions',\n",
       " 'clicked',\n",
       " '“buy',\n",
       " 'wooh',\n",
       " 'advantages',\n",
       " 'bulk',\n",
       " 'purchase',\n",
       " 'needing',\n",
       " 'cycle',\n",
       " 'cycle',\n",
       " 'inserted',\n",
       " 'shhhhh',\n",
       " 'shhhhh',\n",
       " 'bulk',\n",
       " 'manganese',\n",
       " 'multivitamin',\n",
       " 'absorb',\n",
       " 'weighs',\n",
       " '000lb',\n",
       " '140',\n",
       " '400lb',\n",
       " '51',\n",
       " 'callipers',\n",
       " 'aligned',\n",
       " 'occupying',\n",
       " 'buick',\n",
       " 'resolve',\n",
       " 'mutilation',\n",
       " 'colourless',\n",
       " 'workaday',\n",
       " 'dynamics',\n",
       " 'refrigerated',\n",
       " 'shelved',\n",
       " 'silicon',\n",
       " 'sarape',\n",
       " 'sarape',\n",
       " 'kow',\n",
       " 'mediocre',\n",
       " 'venereal',\n",
       " 'prefaced',\n",
       " '“with',\n",
       " 'separation',\n",
       " 'proteins',\n",
       " 'vis',\n",
       " 'vis',\n",
       " 'greaseboards',\n",
       " 'suffice',\n",
       " 'evenly',\n",
       " 'distributed',\n",
       " 'amongst',\n",
       " 'burner',\n",
       " 'eliminating',\n",
       " 'cornered',\n",
       " 'spelunking',\n",
       " 'caves',\n",
       " 'concepts',\n",
       " 'anecdotes',\n",
       " 'dumbed',\n",
       " 'accommodate',\n",
       " 'duration',\n",
       " 'consonants',\n",
       " 'plausible',\n",
       " 'leap',\n",
       " 'depth',\n",
       " 'proctologist',\n",
       " 'surgeon',\n",
       " '“sarcasm”',\n",
       " 'antisocial',\n",
       " 'implication',\n",
       " 'delusion',\n",
       " 'apparent',\n",
       " 'defined',\n",
       " 'causality',\n",
       " 'aforementioned',\n",
       " 'query',\n",
       " 'proximal',\n",
       " 'contradistinction',\n",
       " 'distal',\n",
       " 'interruptus',\n",
       " 'hindered',\n",
       " 'wanders',\n",
       " 'circles',\n",
       " 'clavicle',\n",
       " 'evidently',\n",
       " 'occam',\n",
       " 'committing',\n",
       " 'hostesses',\n",
       " 'fuddruckers',\n",
       " 'yearn',\n",
       " 'downloads',\n",
       " 'spoof',\n",
       " 'bourne',\n",
       " 'restatement',\n",
       " 'scribbled',\n",
       " '“here',\n",
       " 'discouraging',\n",
       " 'parallax',\n",
       " 'distortion',\n",
       " 'louis',\n",
       " 'louise',\n",
       " '212',\n",
       " 'masturbating',\n",
       " 'offline',\n",
       " 'luncheon',\n",
       " 'arabic',\n",
       " 'farsi',\n",
       " 'hurt…',\n",
       " 'mandelbrot',\n",
       " 'chaos',\n",
       " 'organisational',\n",
       " 'flatware',\n",
       " 'straightening',\n",
       " 'swirling',\n",
       " 'vortex',\n",
       " 'immaculate',\n",
       " 'merciful',\n",
       " 'leap',\n",
       " 'derives',\n",
       " 'reflection',\n",
       " 'dolly',\n",
       " 'measurable',\n",
       " 'thou',\n",
       " 'heartless',\n",
       " 'efforts',\n",
       " 'congress',\n",
       " 'unorthodox',\n",
       " 'measurable',\n",
       " 'enhancement',\n",
       " 'organisational',\n",
       " 'schematic',\n",
       " 'otolaryngologist',\n",
       " 'sssshhhh',\n",
       " 'splendidly',\n",
       " 'roused',\n",
       " 'dramatically',\n",
       " 'accelerating',\n",
       " '32',\n",
       " 'swoops',\n",
       " 'reaching',\n",
       " 'approximately',\n",
       " 'sliced',\n",
       " 'rife',\n",
       " 'inaccuracy',\n",
       " 'pantsing',\n",
       " 'intercom',\n",
       " 'denied',\n",
       " 'pathology',\n",
       " 'cuisines',\n",
       " 'gastronomically',\n",
       " 'thailand',\n",
       " 'latter',\n",
       " 'nineteenth',\n",
       " 'interestingly',\n",
       " 'population',\n",
       " 'mack',\n",
       " 'wednesday',\n",
       " 'portray',\n",
       " 'irrational',\n",
       " 'assure',\n",
       " '67',\n",
       " 'divided',\n",
       " 'collective',\n",
       " 'punishable',\n",
       " 'dips',\n",
       " 'walletnook',\n",
       " 'misleading',\n",
       " 'slots',\n",
       " 'removable',\n",
       " 'id',\n",
       " 'removable',\n",
       " 'id',\n",
       " 'restore',\n",
       " 'semblance',\n",
       " 'def',\n",
       " 'enhanced',\n",
       " 'tanks',\n",
       " 'earthquake',\n",
       " 'earthquake',\n",
       " 'reductio',\n",
       " 'absurdum',\n",
       " 'fallacy',\n",
       " 'proportions',\n",
       " 'criticising',\n",
       " 'ablutions',\n",
       " 'accordingly',\n",
       " '11pm',\n",
       " 'ethic',\n",
       " 'estab…',\n",
       " 'myriad',\n",
       " 'hamstrung',\n",
       " 'sh…',\n",
       " 'sportsmanship',\n",
       " 'respawned',\n",
       " 'cornered',\n",
       " 'hmmph',\n",
       " 'huskers',\n",
       " 'paste',\n",
       " 'oriented',\n",
       " 'headboard',\n",
       " 'imperative',\n",
       " 'oneself',\n",
       " 'marauders',\n",
       " 'awakened',\n",
       " 'poured',\n",
       " 'bbc',\n",
       " 'awakened',\n",
       " 'poured',\n",
       " 'cereal…',\n",
       " 'beheaded',\n",
       " 'd…',\n",
       " 'befriends',\n",
       " 'mocked',\n",
       " 'fictional',\n",
       " 'comb',\n",
       " 'embodiment',\n",
       " 'resemblance',\n",
       " 'woven',\n",
       " 'ebony',\n",
       " 'libre',\n",
       " 'sezchuan',\n",
       " 'province',\n",
       " 'slacks',\n",
       " 'disembodied',\n",
       " 'saturdays',\n",
       " 'lectured',\n",
       " 'goosebumps',\n",
       " 'universality',\n",
       " 'transcends',\n",
       " 'ethnicity',\n",
       " 'norm',\n",
       " 'involvement',\n",
       " 'aspiration',\n",
       " 'profession',\n",
       " 'nineteenth',\n",
       " 'norm',\n",
       " 'reseda',\n",
       " 'exceptions',\n",
       " 'libre',\n",
       " 'libre',\n",
       " 'traditionally',\n",
       " 'bartenders',\n",
       " 'mulling',\n",
       " 'inconsiderate',\n",
       " 'adjective',\n",
       " 'inconsiderate',\n",
       " 'courtship',\n",
       " 'eclipse',\n",
       " 'coincide',\n",
       " 'gangster',\n",
       " 'greet',\n",
       " 'refreshing',\n",
       " 'wraps',\n",
       " 'tampered',\n",
       " 'chrono',\n",
       " 'dynamics',\n",
       " 'surpasses',\n",
       " 'decker',\n",
       " 'decker',\n",
       " 'satisfying',\n",
       " 'bun',\n",
       " 'condiment',\n",
       " 'ratio',\n",
       " 'delusion',\n",
       " 'annihilated',\n",
       " 'incompetence',\n",
       " 'fragged',\n",
       " 'troops',\n",
       " 'enquiry',\n",
       " 'concise',\n",
       " 'misjudged',\n",
       " 'limb',\n",
       " 'plantation',\n",
       " 'plantation',\n",
       " 'foreseeable',\n",
       " 'reserving',\n",
       " 'augmented',\n",
       " 'battalion',\n",
       " 'gettysburg',\n",
       " 'eluded',\n",
       " 'semiotics',\n",
       " 'semiotics',\n",
       " 'linguistics',\n",
       " 'constitute',\n",
       " 'manufacture',\n",
       " 'goldfarb',\n",
       " 'aberration',\n",
       " 'beverages',\n",
       " 'eggplants',\n",
       " 'tickled',\n",
       " '08',\n",
       " 'outburst',\n",
       " 'regaled',\n",
       " 'folksy',\n",
       " 'horoscopes',\n",
       " 'avenger',\n",
       " 'flawed',\n",
       " 'neeeeoooowwwww',\n",
       " 'lewis',\n",
       " 'genre',\n",
       " 'manga',\n",
       " 'roman',\n",
       " 'gods…',\n",
       " 'faa',\n",
       " 'analysing',\n",
       " 'wreckage',\n",
       " 'newcomer',\n",
       " 'existing',\n",
       " 'greeting',\n",
       " 'approving',\n",
       " 'chorus',\n",
       " '“dude',\n",
       " 'neeeeooooowwwww',\n",
       " 'neeeeeoooooowwwww',\n",
       " 'neeeeeooooowwwwww',\n",
       " 'dwarves',\n",
       " 'disrupting',\n",
       " 'jane',\n",
       " 'goodall',\n",
       " 'observing',\n",
       " 'unstructured',\n",
       " 'patterns',\n",
       " 'apparent',\n",
       " 'frequency',\n",
       " 'observer',\n",
       " 'sloppy',\n",
       " 'costuming',\n",
       " 'dereliction',\n",
       " 'civet',\n",
       " 'describes',\n",
       " 'quid',\n",
       " 'quo',\n",
       " 'compensation',\n",
       " 'mrow',\n",
       " 'civet',\n",
       " 'cartesian',\n",
       " 'crouch',\n",
       " 'incessantly',\n",
       " 'debakey',\n",
       " 'transplants',\n",
       " 'fleming',\n",
       " 'hairdresser',\n",
       " '“thanks',\n",
       " 'bouffant',\n",
       " 'amputated',\n",
       " 'transplanted',\n",
       " '“problem',\n",
       " 'solved”',\n",
       " 'individuals',\n",
       " 'comparatively',\n",
       " 'workaday',\n",
       " 'unassailable',\n",
       " 'speculate',\n",
       " 'tactical',\n",
       " 'stargate',\n",
       " 'athens',\n",
       " 'sparta',\n",
       " 'unacceptable',\n",
       " 'strategy',\n",
       " 'hesitant',\n",
       " 'unanticipated',\n",
       " 'endorphins',\n",
       " 'satisfying',\n",
       " 'hopped',\n",
       " 'parameter',\n",
       " 'hoberman',\n",
       " 'sample',\n",
       " 'flecked',\n",
       " 'pyrite',\n",
       " 'hoberman',\n",
       " 'collapsible',\n",
       " 'icosidodecahedron',\n",
       " 'novelty',\n",
       " 'precise',\n",
       " 'panel',\n",
       " 'millimetre',\n",
       " 'panel',\n",
       " 'unescorted',\n",
       " 'onetime',\n",
       " 'permanent',\n",
       " 'easement',\n",
       " 'easement',\n",
       " 'marshal',\n",
       " 'concentration',\n",
       " 'assistance',\n",
       " 'revenue',\n",
       " 'optimized',\n",
       " 'viable',\n",
       " '52',\n",
       " 'forfeit',\n",
       " 'diva',\n",
       " 'alphabetical',\n",
       " 'insert',\n",
       " 'usb',\n",
       " 'nummy',\n",
       " 'nummy',\n",
       " 'usb',\n",
       " 'bailment',\n",
       " 'describes',\n",
       " 'possession',\n",
       " 'chattels',\n",
       " 'bailor',\n",
       " 'bailee',\n",
       " 'contents',\n",
       " 'irrelevant',\n",
       " 'bailment',\n",
       " 'deputized',\n",
       " 'entrusted',\n",
       " 'indemnified',\n",
       " 'liable',\n",
       " 'discussions',\n",
       " 'bioorganic',\n",
       " 'devices',\n",
       " 'threaded',\n",
       " 'roundtable',\n",
       " 'nonequilibrium',\n",
       " 'photoionization',\n",
       " 'outlaws',\n",
       " 'voided',\n",
       " 'warranty',\n",
       " 'warranty',\n",
       " 'manufacturer',\n",
       " 'hardware',\n",
       " 'anarchy',\n",
       " 'simplifying',\n",
       " 'attaching',\n",
       " 'rfid',\n",
       " 'tags',\n",
       " 'destination',\n",
       " 'anticipated',\n",
       " 'spectrum',\n",
       " 'duration',\n",
       " 'anisotropy',\n",
       " 'cemented',\n",
       " 'anarchy',\n",
       " 'conclude',\n",
       " 'tastings',\n",
       " 'portland',\n",
       " 'starlight',\n",
       " 'refurbished',\n",
       " '1956',\n",
       " 'theatre…',\n",
       " 'luis',\n",
       " 'obispo',\n",
       " 'minimal',\n",
       " 'repeated',\n",
       " 'unmitigated',\n",
       " 'astrophysical',\n",
       " 'railways',\n",
       " 'ranakpur',\n",
       " 'kilometer',\n",
       " 'maharashtra',\n",
       " 'bikaner',\n",
       " 'junction',\n",
       " 'reap',\n",
       " 'endorphic',\n",
       " 'disposition',\n",
       " 'abate',\n",
       " 'wheelbase',\n",
       " '1980',\n",
       " 'superliner',\n",
       " 'passenger',\n",
       " 'starlight',\n",
       " 'routes',\n",
       " 'panoramic',\n",
       " 'vistas',\n",
       " 'inaccessible',\n",
       " '350',\n",
       " ...]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_by_sheldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43949    False\n",
       "43948    False\n",
       "43947    False\n",
       "43936    False\n",
       "43946    False\n",
       "         ...  \n",
       "47704    False\n",
       "47703    False\n",
       "47702    False\n",
       "47720    False\n",
       "51291    False\n",
       "Name: Line, Length: 10207, dtype: bool"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.Line.str.split(\n",
    "    ).apply(lambda tokens:\n",
    "            all(token in words_by_sheldon for token in tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005878318800822965"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split each line into lists and see if the words are in the list of words said by sheldon\n",
    "(X_test[\n",
    "    X_test.Line.str.split(\n",
    "    ).apply(lambda tokens:\n",
    "            all(token in words_by_sheldon for token in tokens))\n",
    "].Character == 'Sheldon'\n",
    ").sum()/len(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43949    False\n",
       "43948    False\n",
       "43947    False\n",
       "43936    False\n",
       "43946    False\n",
       "         ...  \n",
       "47704    False\n",
       "47703    False\n",
       "47702     True\n",
       "47720    False\n",
       "51291     True\n",
       "Name: Character, Length: 10207, dtype: bool"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_test.Character == 'Sheldon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43949    False\n",
       "43948    False\n",
       "43947    False\n",
       "43936    False\n",
       "43946    False\n",
       "         ...  \n",
       "47704    False\n",
       "47703    False\n",
       "47702    False\n",
       "47720    False\n",
       "51291    False\n",
       "Name: Line, Length: 10207, dtype: bool"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.Line.str.split().apply(lambda tokens:\n",
    "                              all(token in words_by_sheldon for token in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43949    False\n",
       "43948    False\n",
       "43947    False\n",
       "43936    False\n",
       "43946    False\n",
       "         ...  \n",
       "47704    False\n",
       "47703    False\n",
       "47702    False\n",
       "47720    False\n",
       "51291    False\n",
       "Length: 10207, dtype: bool"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_test.Line.str.split().apply(lambda tokens:\n",
    "                              all(token in words_by_sheldon for token in tokens)) & (X_test.Character == 'Sheldon'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43949     True\n",
       "43948     True\n",
       "43947     True\n",
       "43936     True\n",
       "43946     True\n",
       "         ...  \n",
       "47704     True\n",
       "47703     True\n",
       "47702    False\n",
       "47720     True\n",
       "51291    False\n",
       "Length: 10207, dtype: bool"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_test.Line.str.split().apply(lambda tokens:\n",
    "                              all(token in words_by_sheldon for token in tokens)) == (X_test.Character == 'Sheldon'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7438032722641325"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split each line into lists and see if the words are in the list of words said by sheldon\n",
    "(X_test.Line.str.split().apply(lambda tokens:\n",
    "                              all(token in words_by_sheldon for token in tokens)) == (X_test.Character == 'Sheldon')).sum()/len(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is pretty low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15**. (17.5 points) Now, perform singular value decomposition (SVD) on the training TF-IDF matrix, and calculate a **25-dimensional approximation** for both the training and test TF-IDF matrices (you can do this using scikit-learn's **TruncatedSVD** class). Then, train a logistic regression classifier with 10-fold cross-validation (using the scikit-learn **LogisticRegressionCV** class) on the output of the SVD that given a dialogue line, tells you whether it's by Sheldon or by someone else.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**i)** (7.5 points) Report precision, recall and F1-score for both classes (Sheldon and not-Sheldon), as well as accuracy, of your classifier on the training set and the test set. You need to implement the calculation of the evaluation measures (precision, etc.) yourself -- using the scikit-learn functions for them is not allowed.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**ii)** (5 points) What difference do you observe between the model's scores on the training and test sets? What could you infer from the amount of difference you see? What about the difference between scores on the two classes? Given the performance of your classifier, is Leonard right that the accuracy Sheldon claims is unattainable? What about his suspicions about the lines that Sheldon can and cannot distinguish?\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**iii)** (2.5 points) List 10 of the most extreme false positives and 10 of the most extreme false negatives, in terms of the probabilities predicted by the logistic regression model. What are common features of false positives? What about the false negatives?\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**iv)** (2.5 points) What is the most important feature in the model? What are the 5 most important words in this feature? _Hint: Think of the definition of an SVD, and that you did an SVD on the TF-IDF matrix with dialogue lines as rows and words as columns. You have projected the original data points onto a 25-dimensional subspace -- you need to look at the unit vectors you used for the projection._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "10d059aaca96fc9c3493bab5d1c0141ba04039945f551b22f762be411355277a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
